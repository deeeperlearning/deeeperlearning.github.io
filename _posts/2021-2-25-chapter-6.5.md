# 6.5

## 6.5 Back-Propagation and Other Differentiation Algorithms

- Forward propagation : Input $\boldsymbol x$ 의 정보를 각 layer의 hidden unit에 전달하여 output $\hat \boldsymbol y$ 를 계산하는 과정.
- Back-propagation : Forward propagation의 반대방향으로 gradient를 계산하는 과정

 

### 6.5.1 Computational Graphs

각각의 변수를 하나의 node로 하여 계산과정을 graph 형태로 나타내는 방법.

![_config.yml]({{ site.baseurl }}/assets/ch6/computational graph.png)

위의 그림은 weight decay penalty가 포함된 선형회귀 모형을 computational graph 방식으로 나타낸 그림이다. 즉, $\hat\boldsymbol y = \boldsymbol x^{\top} \boldsymbol w$와  $\lambda \sum_i{w_i^2}$   를 계산하는 과정.

### 6.5.2 Chain Rule of Calculus

Back-propagation은 chain rule을 이용하여 gradient를 효율적으로 계산하는 알고리즘이다.

$\boldsymbol x \in \mathbb{R}^m,\  \boldsymbol y \in \mathbb{R}^n,\ z \in \mathbb{R}$ 이고 $\boldsymbol y = f(\boldsymbol x),\ z = g(\boldsymbol y)$ 일 때, chain rule은 아래와 같이 적힌다.

$$\frac{\partial z}{\partial x_i} = \sum_{j}{\frac{\partial z}{\partial y_j}}\frac{\partial y_j}{\partial x_i}\ \ or\ \ \nabla_{\boldsymbol x}z = \left(\frac{\partial\boldsymbol y}{\partial\boldsymbol x}\right)^{\top}\nabla _{\boldsymbol y}z$$

### 6.5.3 Recursively Applying the Chain Rule to Obtain Back-propagation

- 역전파를 하는 과정에서 chain rule을 적용할 때 동일한 계산을 여러번 하게됨.

    ![_config.yml]({{ site.baseurl }}/assets/ch6/recursive1.png)

- 따라서 역전파의 방향을 따라 각 노드의 gradient 값을 저장하면 반복적인 계산을 피할 수 있다.

    ![_config.yml]({{ site.baseurl }}/assets/ch6/recursive2.png)

- 하지만 gradient를 저장할 메모리가 증가한다는 단점이 있다.

### 6.5.4 Back-Propagation Computation in Fully-Connected MLP

- Forward propagation

![_config.yml]({{ site.baseurl }}/assets/ch6/algo6_3.png)

- Back-propagation
- ![_config.yml]({{ site.baseurl }}/assets/ch6/algo6_4.png)

### 6.5.5 Symbol-to-Symbol Derivatives

- Symbolic representation : 대수 표현과 computational graph와 같이 symbol을 이용하여 계산과정 또는 식을 표현
- 역전파를 진행할 때 computational graph와 입력 데이터의 실제 값(numerical value)을 이용하여 gradient의 실제 값을 도출하는 접근법을 "symbol-to-number"라고 한다.
    - Torch
- Computational graph에 추가적인 노드를 만들고 필요한 미분을 symbolic하게 저장하는 방식을 "symbol-to-symbol"이라고 한다.
    - TensorFlow

    ![_config.yml]({{ site.baseurl }}/assets/ch6/sym2sym.png)

    - 이러한 미분들은 하나의 computational graph를 만들기 때문에 또 다시 역전파를 진행할 수 있고, 즉, higher-order의 미분을 얻을 수 있다.
- "symbol-to-symbol"이 "symbol-to-number"보다 넓은 개념.

### 6.5.6 General Back-Propagation

$\mathcal{G}$ : computational graph.

$\mathsf{V}$ : variable(텐서) ($\mathcal G$에서 노드에 해당) .

get_operation($\mathsf V$) : $\mathsf V$를 계산하는 오퍼레이터를 리턴.

get_consumer($\mathsf V, \mathcal G$) : $\mathsf V$의 자식 변수를 리턴.

get_inputs($\mathsf V, \mathcal G$) : $\mathsf V$의 부모 변수를 리턴.

op.bprop($\text{inputs},\mathsf X, \mathsf G$) : $\sum_i(\nabla_{\mathsf X}\text{op.f(inputs)}_i)\mathsf G_i$ (chain rule)

- 역전파 알고리즘

    ![_config.yml]({{ site.baseurl }}/assets/ch6/algo6_5.png)

- build_grad 알고리즘

    ![_config.yml]({{ site.baseurl }}/assets/ch6/algo6_6.png)

- n개의 노드를 가진 computational graph는 directed acyclic graph이기 때문에 gradient를 계산하기 위한 cost는 최대 $O(n^2)$이다.

### 6.5.7 Example: Back-Propagation for MLP Training

아래와 같이 히든 레이어가 하나인 퍼셉트론 문제를 생각해보자.

![_config.yml]({{ site.baseurl }}/assets/ch6/MLP.png)

$$J = J_{MLE}+\lambda\left(\sum_{i,j}\left(W_{i,j}^{(1)}\right)^2+\sum_{i,j}\left(W_{i,j}^{(2)}\right)^2\right)$$

$\nabla_{\boldsymbol W^{(1)}}J$ 와 $\nabla_{\boldsymbol W^{(2)}}J$ 은 두가지 길(weight decay cost와 cross entropy cost)을 통해 계산할 수 있다.

- Weight decay cost
    - 단순히 $2\lambda \boldsymbol W^{(i)}$로 주어짐.
- Cross entropy cost
    - $\boldsymbol U^{(2)}$에 대한 gradient를 $\frac{\partial J_{MLE}}{\partial \boldsymbol U^{(2)}} = \boldsymbol G$라 하면 $\boldsymbol W^{(2)}$에 대한 gradient는 아래와 같다.

    $$\nabla_{\boldsymbol W^{(2)}}J_{MLE} = \boldsymbol H^{\top} \boldsymbol G$$

    - $\boldsymbol U^{(1)}$에 대한 gradient를 $\frac{\partial J_{MLE}}{\partial \boldsymbol U^{(1)}} = \boldsymbol G^{\prime}$라 하면(relu가 있기 때문에 $\boldsymbol U^{(1)}$의 component 중 0 보다 작은 애들은 gradient를 0으로) $\boldsymbol W^{(1)}$에 대한 gradient는 아래와 같다.

    $$\nabla_{\boldsymbol W^{(1)}} J_{MLE} = \boldsymbol X^{\top} \boldsymbol G^{\prime}$$

### 6.5.8 Complications

- 어떤 operator가 두개의 텐서를 리턴할 경우, 두개의 출력이 있는 단일 작업으로 구현하는 것이 가장 효율적이다.
- 역전파 과정에서 여러개의 텐서를 더하는 경우가 많은데, 각각의 텐서를 따로 계산한 후에 전체를 더하게 되면 메모리 병목현상이 발생함. 따라서 하나의 버퍼를 유지하면서 하나씩 더하는게 메모리 적약에 효율적이다.
- 역전파과정은 여러가지 데이터 타입을 다뤄야 하기 때문에 데이터 타입별 처리 방식을 잘 디자인 해야한다.
- gradient가 정의되지 않는 operator가 있을 경우 사용자가 알 수 있도록 tracking이 잘 되어야 한다.

### 6.5.9 Differentiation outside the Deep Learning Community

- 역전파 알고리즘은 자동미분의 접근법중 하나일 뿐이고, 다른 방법들도 존재한다.
- 자동미분에는 대표적으로 forward/reverse mode accumulation이 존재하는데 역전파 알고리즘은 reverse mode에 속함.
    - reverse mode accumulation (output에서 input 방향)

        $$\frac{\partial y}{\partial x} = \frac{\partial y}{\partial w_1}\frac{\partial w_1}{\partial x} = \left(\frac{\partial y}{\partial w_2}\frac{\partial w_2}{\partial w_1}\right)\frac{\partial w_1}{\partial x}= \left(\left(\frac{\partial y}{\partial w_3}\frac{\partial w_3}{\partial w_2}\right)\frac{\partial w_2}{\partial w_1}\right)\frac{\partial w_1}{\partial x}$$

    - forward mode accumulation (input에서 output 방향)

        $$\frac{\partial y}{\partial x} = \frac{\partial y}{\partial w_{n-1}}\frac{\partial w_{n-1}}{\partial x} = \frac{\partial y}{\partial w_{n-1}}\left(\frac{\partial w_{n-1}}{\partial w_{n-2}}\frac{\partial w_{n-2}}{\partial x}\right)= \frac{\partial y}{\partial w_{n-1}}\left(\frac{\partial w_{n-1}}{\partial w_{n-2}}\left(\frac{\partial w_{n-2}}{\partial w_{n-3}}\frac{\partial w_{n-3}}{\partial x}\right)\right)$$

- Chain rule을 적용할 때 미분의 순서를 최적화시키는 문제는 NP-complete 문제로 알려져있다.
    - TensorFlow나 Theano는 휴리스틱 알고리즘을 사용해서 computational graph를 간소화 하는 방식으로 cost를 줄인다고 합니다.
- 머신러닝 분야는 특이하게 특정 라이브러리를 사용해서 코드를 짜기 때문에 많은 제약이 따르는데, 역전파 알고리즘을 커스텀할 수 있다는 장점도 있다.

### 6.5.10 Higher-Order Derivatives

- 사실 high order 미분이 딥려닝에서 자주 쓰이진 않고, Hessian 행렬의 몇가지 특징들 때문에 가끔 사용된다.
- 하지만 Hessian 행렬을 직접 계산하는 것은 cost가 너무 크기 때문에 Krylov method를 사용해서 간접적으로 구한다.

    $$\boldsymbol H \boldsymbol v = \nabla_{\boldsymbol x}\left[(\nabla_{\boldsymbol x}f(\boldsymbol x))^{\top}\boldsymbol v\right]$$

    - outer gradient는 inner gradient에만 적용됨.