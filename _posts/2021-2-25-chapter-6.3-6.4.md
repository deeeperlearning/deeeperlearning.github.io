# 6.3 Hidden Units

- Hidden units에 대해서는 절대적인 이론적 가이드라인이 있는게 아니라, 다양한 설계가 연구중임
- Rectified linear unit (ReLU)가 기본적인 hidden unit으로 널리 쓰임
- 소개할 몇몇 hidden unit은 미분 불가능한 지점을 포함하기도 함
  - Ex) $g(z)=max{0,z}$ 는 $z=0$에서 미분 불가능함
  - 그러나 이러한 경우가 많지는 않으며, 소프트웨어에서 인위적으로 특정 값을 배정하여 수치연산 오류를 방지하곤 함

# Rectiﬁed Linear Units and Their Generalizations

- Actiation function: $g(z)=max{0,z}$
  - Linear unit과 유사하기 때문에 최적화 하기 용이함 (차이점: 절반의 결과값이 0이 됨)
  - 미분값은 active unit의 경우 상수로 일정하고, 이차미분값은 모두 0임
- 일반적으로 affine matrix를 input으로 이용: $h=g(W^⊤x+b)$
  - 보통 $b=0.1$과 같이 작은 양수로 초기값을 설정 $\rightarrow$ 대부분의 unit이 초기 상태부터 active하도록 함 
- ReLU 보다 성능이 높도록 개량된 버전의 예시
  - $h_i=g(z,\alpha)_i=max(0,z_i)+\alpha_imin(0,z_i)$ 에서,
    - $\alpha_i=-1$ : 절대값을 계산하는 absolute value rectification $g(z)=|z|$
    - $\alpha_i$가 0.01과 같이 작은 값으로 고정: leaky ReLU
    - $\alpha_i$가 학습 가능한 parameter: parametric ReLU, PReLU
    - Maxout unit: ReLU를 좀 더 변형 $\rightarrow$ 여러 input 중 가장 큰 값을 내뱉음


# Logistic Sigmoid and Hyperbolic Tangent

- ReLU 이전에는 대부분 sigmoid activation function, hyperbolic tangent activation function을 사용함
  - Sigmoid: $g(z)=\sigma(z)$
  - Hyperbolic tangent: $g(z)=tanh(z)$
  - $tanh(z)=2\sigma(2z)-1$ 이기 때문에 두 함수는 유사하게 작동함
  - 이 둘은 saturation되어 hideen unit으로 잘 사용되지는 않음
  - 다만, saturation을 상쇄해주는 cost function이 함께 있을 때, output 함수로 종종 사용되기도 함
- Sigmoid 보다는 hyperbolic tangent를 사용하는 것이 더 나음
  - $\sigma(0)=0.5$이지만, $tanh(0)=0$이기 때문에, 0 근처에서 y=x와 더 가깝게 작용하여 더 간단하게 학습하는 모델이 됨
  - Recurrent network, probabilistic model, autoencoder 등에서 특정 제한 조건이 있는 경우, saturation이라는 단점에도 불구하고 sigmoid function을 사용하기도 함
    - 예) Firing rate의 상한이 있는 뉴런의 활동을 모사


# Other Hidden Units

- 다른 hidden unit들도 다양하지만, 잘 쓰이지는 않음
  - Cosine function을 이용했을 때 MNIST 벤치마크 데이터셋에서 테스트 성능이 1퍼센트 향상됨
  - Softmax는 output으로는 흔히 쓰이지만 hidden unit으로는 잘 쓰이지 않음  (10장에서 다룰 예정)
  - radial basis function (RBF), Softplus, Hard tanh 등 최근까지도 다양한 함수들이 제시되며 활발히 연구되고 있는 분야임


# 6.4 Architecture Design

- Neural network은 여러 개의 레이어로 구성되어 있는데, 이를 어떤 너비, 얼마만큼의 깊이로 쌓을지 결정해야 함
- 각 레이어는 이전 레이어의 출력이 다음 레이어의 입력이 되는 chain-based 구조를 가짐


# Universal Approximation Properties and Depth

- Feature과 output을 행렬곱으로 맵핑하는 linear model은 convex optimization 을 사용하여 해결할 수 있지만, 우리가 학습하려는 대상은 nonlinear한 함수일 수 있음
- Hidden layer가 있는 feedforward network는 어떠한 함수든 근사할 수 있다는 universal approximation theorem에서 설명하듯, nonlinear 한 함수도 잘 학습할 수 있음
- MLP가 함수를 표현할 수 있다고 해도 학습이 실패할 수 있는 두 가지 이유가 있음
  - 최적화 알고리즘이 학습해야할 함수에 적합하지 않을 수 있음
  - 학습 알고리즘이 overfitting 되어 엉뚱한 함수를 학습할 수 있음
- "공짜 점심은 없다" 이론에서 언급했듯이, 모든 것을 다 만족시키는 만능 AI란 없음
  - Universal approximation theorem이란 주어진 함수를 MLP를 이용해 표현 가능하다는 거지, 학습한 함수가 test set에 대해서도 만능으로 잘 작동한다는 뜻은 아님
- 따라서 parameter 수를 줄여가며 원하는 함수를 representation 해야 함
  - 아래 그림) 레이어를 깊게 쌓아감에 따라 decision boundary가 단순해지는 에시
    - Activation function은 절댓값을 사용하였기 때문에 레이어 하나를 지날 때마다 반절씩 접는 것으로 표현함
    - Affine transformation은 반절 접는 위치를 정해줌
    - 레이어를 지날수록 근사해야하는 함수가 단순해지고, 그렇기 때문에 generalization error가 줄어든다고 볼 수 있음
<Fig 6.5>
- 어느 지점까지는 network의 depth, size가 확보되어야 하지만, 무조건 크다고 성능이 향상되는 것은 아님 (아래 두 그림 예시)
<Fig 6.6, 6.7>


# Other Architectural Considerations

- 여기까지는 neural network를 단순히 연결로만 보고 레이어의 깊이와 너비(유닛 수)에 대해서 주로 논했지만, 사실은 목적에 따라 더 다양함
  - Convolution network (9장)
  - Sequence 데이터를 처리하기 위해 만들어진 recurrent network
- 기본적으로 main chain을 어떻게 설계하는지가 중요하지만, 이 외에도 skip connection, attention, convolutional filter 등의 설계 역시 중요함
  - Skip connection: $i$번째 레이어에서 $i+1$번째가 아닌 $i+2$번째 레이어로 바로 넘겨서 gradient flow를 용이하게 만드는 기법
  - Attention: Representation 의 일부만 선택적으로 넘겨서 효율을 향상시키는 방법
  - Convolutional network: 도메인 내에서 filter를 공유함으로써 적은 파라미터로도 복잡한 패턴을 효율적으로 잡아낼 수 있게 설계된 방법
