# 제목 없음

### 15.1.1 When and Why Does Unsupervised Pretraining Work?

Greedy layer-wise 비지도 사전학습은 분류 작업에서 큰 성능향상을 보여주지만 다른 작업에서는 오히려 해로운 효과를 야기할 수 있기 때문에, '언제' 그리고 '왜' 이 방법이 성능향상을 가져오는지 알아야 한다.

비지도 사전학습은 두가지 아이디어를 내포하고 있다.

- 심층학습 네트워크에 사용될 파라미터의 초기값에 따라 정규화 효과를 기대할 수 있다.
    - 아직 파라미터 초기값 설정에 대한 이해가 부족하고 deep neural network가 완벽한 minimum에 데려다 주지 않는다는 것을 안다.
    - 하지만, 사전학습이 우리가 도달하지 못하는 영역에 데려다 줄 수 있다는 가능성은 있다.

        ![_config.yml]({{ site.baseurl }}/assets/ch15/Fig15_1.png)

        위 그림은 Erhan et al. (2010)의 실험 결과이다. 서로다른 초기값(각각의 점)에서 시작한 모델들의 시간에 따른 학습 궤적을 function space에 나타낸 것이다. 색깔은 시간을 의미한다. 모든 점들은 그림의 중앙에서 시작하여 바깥쪽으로 뻗어나가는데 사전학습을 한 경우에는 궤적의 분산이 매우 작다. 즉, 뉴럴넷의 stochasticity를 줄일 수 있다.

- 입력 데이터의 분포를 학습하는 것은 입력과 출력 사이의 mapping을 학습하는데 도움이 된다.
    - 비지도 학습에서 유용한 feature는 지도 학습에서도 유용할 것이다. 예를 들어, 자동차와 오토바이를 분류하기 위해선 바퀴의 특징과 바퀴의 개수를 파악할 수 있어야하는데, 비지도 사전학습이 바퀴에 대한 좀 더 쉬운 representation을 찾아준다면 분류가 쉬워질 것이다.
    - 하지만, 아직 수학적으로 증명된 것이 아니고 어떤 모델을 사용하는가에 대한 의존성도 너무 크다. 예를 들어, 지도학습에 선형 분류기를 사용한다면 사전학습된 feature는 학습할 클래스들을 선형적으로 분류할 수 있어야 한다.

또한 비지도 사전학습을 아래와 같이 두가지 관점으로 생각해 볼 수 있다.

- Representation 학습으로 바라보는 관점
    - 초기의 representation이 좋지 않을수록 큰 효과를 기대할 수 있다.  One-hot 벡터로 단어를 embedding 시킨다면 인접한 두 단어 사이의 거리는 항상 일정하지만, 학습된 word embedding은 자연스럽게 두 단어 사이의 유사성을 학습하게 된다. 이런 경우, 비지도 사전학습은 좋은 효과를 기대할 수 있다.
- Regularizer로 바라보는 관점
    - 비지도 사전학습을 통해 추가된 정보는 모두 라벨이 없는 데이터로부터 왔기 때문에 라벨이 없는 데이터가 많을수록 큰 효과를 기대할 수 있다.
    - 또한, Weight decay 같은 정규화와는 다르게 비지도 학습은 feature functions을 학습하기 때문에, 학습해야 할 함수가 굉장히 복잡할 때 이로울 수 있다.

비지도 사전학습을 통해 위와 같은 이로운 효과를 기대할 수 있지만, 학습을 두 스테이지로 나눔으로써 생기는 단점도 있다.

- 정규화 효과에 관여하는 하이퍼파라미터가 너무 많고, 학습이 끝난 후에야 측정이 가능하다.
- 비지도 사전학습에 사용되는 하이퍼파라미터를 튜닝하기 위해서는 지도학습까지 끝내야 하므로 시간이 오래걸린다.

사실 요즘에는 dropout, batch normalization 등등 좋은 기술들이 많고 성능도 더 좋아서 비지도 사전학습은 NLP쪽 빼고는 거의 사용하지 않는다고 한다...