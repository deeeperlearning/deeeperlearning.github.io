# 제목 없음

## 12.5 Other Applications

이번 단원에서는 이전에 다루었던 사물인식, 음성인식 그리고 자연어 처리과정과는 다른 딥러닝 응용기술을 다룬다.

### 12.5.1 Recommender Systems

정보 기술 분야에서 중요한 task중 하나는 온라인상에서 광고나 상품을 추천해주는 시스템이다. 추천 시스템의 발전 초기에는 상품(또는 광고)을 특정 사용자에게 추천해기 위한 최소한의 정보를 사용했다. 예를들어, 사용자 1과 사용자 2가 상품 A, B, C를 모두 좋아할때, 사용자 1이 상품 D를 좋아하면 사용자 2도 상품 D를 좋아할 가능성이 크다. 이러한 원리를 이용한 알고리즘을 collaborative filtering 이라고 한다. 하나의 방법으로, 사용자와 상품을 embedding하여 예측 값을 얻어내는 방법이 있다.

사용자를 $\boldsymbol A$ 행렬의 열에, 상품을 $\boldsymbol B$ 행렬의 행에 임베딩 시키고, $\boldsymbol b$와  $\boldsymbol c$ 를 사용자와 상품의 바이어스 벡터로 두면 예측값은 아래와 같이 나타낼 수 있다.

$$\hat R_{u,i} = b_{u}+ c_{i}+\sum_{j}A_{u,j}B_{j,i}$$

일반적으로는 구해진 예측값 $\boldsymbol{\hat R}$ 과 실제 레이팅 $\boldsymbol R$의 square error를 최소화 시켜서 학습한다.

또 다른 방법으로 SVD를 이용하여 $\boldsymbol R = \boldsymbol{UDV'}$으로 표현하고, 사용자와 상품의 행렬을 $\boldsymbol A = \boldsymbol{UD}$, $\boldsymbol B=\boldsymbol V'$으로 적는 방법도 있다. (실제로 두 방법 모두 Netflix prize에서 좋은 성능을 보여주었다고 한다.)

하지만 이런 collaborative filtering 시스템은 새로운 상품(사용자)이 들어왔을 때 이에 대한 rating이 없기에 다른 상품(사용자)와의 유사성을 계산할 수 없다는 단점이 있다. 이러한 'cold-start recommendation'을 해결하기 위해 사용자 약력 정보나 상품의 특징 정보를 추가적으로 이용한다(content-based recommender systems).

### 12.5.1.1 Exploration Versus Exploitation

추천 시스템을 만들 때, 지도학습에서 강화학습으로 넘어가는 문제가 발생한다.

학습을 위한 데이터를 모으기 위해 인터넷에 들어가면 이미 추천시스템이 적용 된 데이터만 볼 수 있기 때문에 바이어스가 큰 데이터를 얻게되고, 결국 다른 항목을 추천했을 때 발생하는 상황에 대한 정보는 얻지 못한다. 또한 추가적인 데이터를 조심해서 구하지 않으면, 데이터가 많아질수록 추천 시스템은 계속해서 잘못된 결정을 내릴 것이다(옳은 결정은 매우 낮은 확률값을 가지고, 시스템이 옳은 결정을 하지 않는한 학습이 안되기 때문). 따라서 옳은 결정을 할 때만 보상이 주어지는 강화학습과 비슷하다.

강화학습은 탐색과 착취 사이의 균형이 잘 맞아야한다. a라는 행동을 했을 때 1 이라는 보상이 주어진다는 사실을 안다고 할 때, 착취란 행동 a를 실행하여 보상을 얻는 것을 의미하고 탐색이란 어떤 보상이 주어질지 모르는 새로운 행동을 하여 지식을 얻는 과정이다. 어떤 행위자가 보상을 받기까지 긴 시간 기다릴 수 있다면 탐색을, 기다릴 수 없다면 착취를 선택하는것이 합리적일 것이다.