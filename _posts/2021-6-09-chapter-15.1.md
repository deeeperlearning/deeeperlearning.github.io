- 이 단원에서는 representation을 학습한다는 것의 의미와 이 개념이 깊은 신경망 구조를 디자인하는데 왜 유용한지에 대해 다룰 예정임

   - 학습 알고리즘이 통계적인 정보를 어떻게 다른 과제 사이에서 공유하는지

   - 비지도 학습에서 얻어진 정보를 어떻게 지도 학습 과제를 위해 사용하는지



- Representation을 공유하는 것은 여러 도메인을 한 번에 다루거나, 학습된 정보를 훈련용 데이터가 부족한 과제에 적용시킬 때 도움이 됨

   - 이는 distributed representation(Hinton et al., 1986)에서부터 시작된 논의로, 15.4에서 다룰 예정임



- 정보를 어떻게 표현하는지에 따라 기계학습 과제는 매우 쉬워질수도, 매루 어려워질수도 있음

   - 예1) 210을 6으로 나눌 때 long 타입을 적용한다면?

   - 예2) 숫자를 sorted list의 알맞은 위치에 집어넣을 때,
      
      - Linked list라면 $O(n)$의 연산이 필요함
      
      - Red-black tree라면 $O(log n)$의 연산이 필요함



- 그렇다면, 무엇이 좋은 representation일까? $\rightarrow$ 과제를 쉽게 만드는 representation

   - 따라서 과제에 따라 알맞은 representation도 변하게 됨


- 예를 들어, 지도 학습으로 훈련된 앞먹임 신경망을 representation 학습이라 할 수 있음

   - 마지막 층은 softmax regression 분류기와 같은 선형 분류기이고, 나머지는 이 분류기에 제공하기 위한 representation을 학습함

   - 지도 학습 과정은 자연스럽게 모든 은닉 계층에 representation을 발생시킴 (상위의 은닉 계층에 더욱 많은 정보가 저장됨)
   
   - 따라서 인풋에서는 선형적으로 separable 하지 않던 클래스들이, 마지막 은닉 계층에서는 separable 해질 수 있음



- 대부분의 representation 학습는 최대한 많은 정보의 보존과 좋은 성질을 획득하는 작업 사이의 tradeoff를 맞게됨
   
   - 비지도 혹은 semi-지도 학습이 가능하도록 하는 점 또한 representation 학습의 흥미로운 부분임

   - 일반적으로 많은 양의 unlabeled, 적은 양의 labeled 학습 데이터를 이용하게 됨

   - 적은 양의 labeled 데이터에 지도 학습을 적용하면 심각한 overfitting이 발생하기도 함

   - 이 때, semi-지도 학습은 unlabel 데이터에서부터도 학습을 함으로서 overfitting 문제를 해결할 수도 있음

   - 요약하면, unlabeled 데이터에서부터 좋은 representation을 학습하고, 이를 지도 학습 과제에 적용하게 됨



- 인간이나 동물은 매우 적은 양의 labeled 데이터에서도 학습이 가능한데, 어떻게 가능한지는 알려진 바가 없음

   - 뇌가 비지도, 혹은 semi-지도 학습을 이용하여 레버리지를 작동시킬 수도 있음

   - Unlabeled 데이터로부터 좋은 representation을 학습하는 레버리지를 작동시키는 여러 방법에 대해서 다룰 예정임



## 15.1 Greedy Layer-Wise Unsupervised Pretraining

- Greedy layer-wise unsupervised pretraining : 합성곱이나 recurrence와 같은 특수한 구조가 없이도 지도 학습을 위한 신경망을 학습시키는 방법

   - 한 과제에서 학습한 representation(인풋 분포를 파악하는 비지도 학습)을 다른 과제에 이용(같은 인풋 도메인에 대한 지도 학습)함



![_config.yml]({{ site.baseurl }}/assets/ch15/Greedy.PNG)



- "Greedy ... pretraining"은 한 계층에 대한 학습 알고리즘으로 작동함 (RBM, single-layer autoencoder 등)

   - 각 계층은 비지도 학습을 이용해 pretraining되고, 이전 계층의 아웃풋으로부터 데이터의 새로운 (그리고 아마도 간단한) representation을 만들어냄

   - 지도 학습 과제를 위한 신경망에 접목시키는 과정이 어렵다고 여겨져왔으나, 2000년대에 접어들어 딥러닝 르네상스를 맞으며 좋은 initialization 조건을 만드는 방법으로 관심받고 있음



- "Greedy(탐욕스러운)"이라고 불리는 이유는 솔루션의 모든 조각들을 독립적으로 최적화하기 때문임

   - "layer-wise"는 이 각각의 독립된 조각들이 신경망의 개별 계층들이기 때문임

   - 다시 말해 한 번에 한 계층씩 작동하며, 이 때 모든 이전 계층들은 고정되어 있음

   - 각 계층이 비지도 학습 알고리즘으로 학습되어 "비지도"가 붙지만, 동시에 labeled 데이터를 이용한 fine-tuning을 실행하기 전이므로 pretraining이 붙음
