# 제목 없음

## 15.4 Distributed Representation

분산 표현은 $n$개의 특징을 $k$개의 값으로 나타냄으로써 $k^n$개의 개념을 설명할 수 있다. 많은 딥러닝 모델들이 "히든 유닛이 주어진 데이터를 설명하는 근본적인 요소를 학습할 수 있다" 라는 가정에 영감을 받았기 때문에, 분산 표현을 사용하는 것은 매우 자연스러운 접근이다. 아래 그림을 보면 비분산 또는 희소 표현 과 분산 표현의 차이를 알 수 있다.

![_config.yml]({{ site.baseurl }}/assets/ch15/Fig_dist_rep.png)

([https://towardsdatascience.com/distributed-vector-representation-simplified-55bd2965333e](https://towardsdatascience.com/distributed-vector-representation-simplified-55bd2965333e))

비분산 표현은 일반적으로 class를 분류하지만 분산 표현은 각각의 특징에 값을 부여하여 representation space를 나눈다. 즉, 분산표현은 서로 다른 개념(개, 고양이 ...) 사이의 공유되는 특징을 학습하기 때문에 일반화 관점에서 큰 이점을 얻는다. 예를 들어, 분산 표현이 '모피를 가졌는가' 와 '다리의 개수'라는 특징을 포함한다면, 개와 고양이는 두 특징에 대해 같은 값을 가질 것이다. 또한, 단어를 표현할 때 분산 표현이 one-hot 표현 보다 풍부한 similarity space를 제공하기 때문에, 분산 표현을 사용한 언어 모델이 다른 모델보다 일반화가 잘 된다.

비분산 표현은 학습할 함수 $f$에 대해 '$u\approx v$라면 $f(u)\approx f(v)$일 것이다'라는 가정을 기본으로 한다. 하지만, 

데이터의 차원이 증가하고 조금의 변화($x \rightarrow x+\epsilon$)에 굉장히 민감하다면, 서로 다른 카테고리를 분류하기 위해 굉장히 많은 파라미터가 필요하다. 하지만 분산 표현을 사용하게 되면 이러한 복잡한 구조의 함수도 적은 양의 파라미터로 표현이 가능하다.

![_config.yml]({{ site.baseurl }}/assets/ch15/Fig15_7.png)

위 그림은 분산 표현을 사용하여 representation을 나눈 그림이다($n=3, d=2$). 분산 표현을 사용하면$O(nd)$의 파라미터로 $O(n^d)$ 만큼의 서로 다른 영역을 표현할 수 있다(Zaslavsky(1975), Pascanu et al.(2014b)). 따라서 피팅할 파라미터의 양이 줄고, 즉, 일반화 시키기 위한 training example의 수가 줄게 된다.

Radford et al. (2015)의 실험 결과를 보면 컨볼루션 네트워크에서는 이러한 분산 표현이 (항상은 아니지만) 굉장히 직관적이고 해석 가능한 결과를 준다는 것을 알 수 있다.

![_config.yml]({{ site.baseurl }}/assets/ch15/Fig15_9.png)

위 그림을 보면 Generative 모델을 통해 학습된 분산 표현이 성별과 안경의 유무를 구분한다는 것을 알 수 있다. 안경을 낀 남성 이미지의 representation 벡터에서 남성 이미지의 벡터를 빼고 여성 이미지의 벡터를 더한 새로운 벡터는 안경을 낀 여성의 이미지를 만들어 낸다. 이러한 일반화된 특징은 학습하지 않은 이미지에 대해서도 적용 될 수 있다.