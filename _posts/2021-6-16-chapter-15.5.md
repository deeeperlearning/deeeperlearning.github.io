## 15.5 Exponential Gains from Depth

이 책의 앞에서 신경망이 깊어질수록 얻는 이득에 대해 소개했다. 요약하면 깊은 신경망은 아래와 같은 통계적인 이득을 얻을 수 있다.

- 다층 퍼셉트론의 경우 보편적 근사기이다.
- 일부 함수의 경우 깊은 신경망을 이용하면 얕은 신경망에 더 작은 신경망으로도 같은 함수를 표현할 수 있다.
- 같은 함수를 표현하기 위한 깊은 신경망의 크기는 얕은 신경망에 비해 지수적으로 작다.

이러한 이득은 분산 은닉 표현을 사용하는 다른 종류의 신경망에도 일반적으로 적용된다.

- 15.4 절의 생성 모형
    - 15.4에서는 얼굴 이미지에서 바탕에 깔린 요소들(안경 착용 여부, 성별 등)을 학습한 생성 모형의 예를 보았다.
    - 이러한 인자들은 주로 고차원의 feature들이다.
    - 일반적으로 이러한 고차원 feature들은 모델의 입력과 복잡한 비선형 관계를 이룰 가능성이 크다.
    - 따라서 이러한 고차원 feature들을 표현하기 위해서는 깊은 싱경망을 이용해 여러 인자들을 비선형적으로 조합해야 한다.
- 다수의 비선형성들과 재사용된 feature들을 hiararchical하게 조합하면 통계적 효율성이 지수적으로 증가한다.
    - 물론 (보편적 근사기이미으로) 하나의 은닉층만 가지고도 이를 표현할 수 있지만, 필요한 은닉 단위의 개수가 굉장히 클 수 있다.
- 합성곱 신경망에 대해서 깊은 모델이 가지는 지수적인 이득을 이론적으로 보인 연구도 있다.