## Chapter 16. Structured Probabilistic Models for Deep Learning

- 딥러닝은 알고리즘을 형식적으로 표현하는 formalism에 기반해 발전해왔는데, structured probabilistic model 역시 이 중 하나임

  - Part II에서 간단히 소개한 바에 더해서, Part III에서 structured probabilistic model은 여러 연구 주제에 필수적으로 포함되는 요소임

- Structured probabilistic model은 그래프를 이용해 확률 분포를 표현함으로써, 랜덤 변수들 사이의 상호 작용을 나타내는 방법임

  - 그래프를 이용하기 때문에 graphical model이라고도 불림

  - 이 단원에서는 다양한 graphical model들을 관통하는 중심 아이디어에 대해 소개할 예정임

  - 또한 16.7에서는 graphical model을 딥러닝에 적용하는 몇 가지 특수한 방법들에 대해 소개할 예정임



## 16.1 The Challenge of Unstructured Modeling

- 딥러닝의 핵심 과제 중 하나는 복잡한 구조를 가진 고차원의 데이터를 이해하는 것임 (ex-자연 이미지, 음성 언어 등)


- 분류 알고리즘은 이러한 고차원의 분포에서 인풋을 받아, 라벨이라는 저차원 데이터로 요약함

  - 이 과정에서 인풋에 담겨있는 많은 양의 정보를 무시하거나 버리게 됨 (ex-사진의 물체를 인식하는 도중 배경은 무시함)


- 한 편, probabilistic model을 이용하면 데이터의 전반적인 구조를 좀 더 자세히 이해함으로써, 단순 분류를 넘어 많은 작업들을 수행할 수 있음

  - Density estimation, denosing, missing value imputation, sampling 등

![_config.yml]({{ site.baseurl }}/assets/ch16/Fig16_1.PNG)


- 수백만 개의 랜덤 변수들에 대해 복잡한 (rich) 분포를 모델링하는 것은 어렵고, 많은 양의 연산이 소모되는 작업임

  - Binary 변수에 대한 간단한 모델링에서도, 32 x 32 RGB 이미지라면 전체 경우의 수가 $2^{3072}$ 나 됨

  - $k$개의 값이 가능할 때, $n$개의 변수로 이루어진 벡터 $x$를 만드는 경우의 수는 $k^n$가 되는데, 이를 모두 파악하는 건 아래의 이유로 적절한 방법이 아님

    - Memory, statistical efficiency, runtime (inference, sampling)


- 하지만 현실에서 대처하는 많은 문제에서는 각각의 변수 사이의 상관 관계가 있기 때문에, 위와 같이 모든 조합을 고려하는 table-based 방법을 취할 필요가 없음

  - 예) 릴레이 계주에서 두번째 이후 주자의 통과 기록은 이전 주자의 기록에 영향을 받을 수 밖에 없음

  - 하지만 3번 주자와 1번 주자의 기록 사이의 영향은 간접적(indirect)인데, 2번 주자의 기록에 큰 영향을 받기 때문임

  - 따라서 계주 기록 모델링에서는 1-2, 2-3 주자 사이의 상호 작용만 고려하면 되고, 1-3 주자 사이의 상호 작용은 고려할 필요 없음


- Structured probabilistic model에서는 랜덤 변수 사이의 직접적인(direct) 상호 작용을 파악함

  - 이를 통해 더 적은 양의 데이터에서 추출된 적은 수의 변수를 가지고도, 더 적은 양의 연산으로 모델을 작동할 수 있음



## 16.3 Sampling from Graphical Models

- Directed graphical model의 장점 중 하나는 ancestral sampling을 이용해 간단하게 샘플을 만들을 만들어낼 수 있다는 것임

  - Ancestral sampling: 변수들의 순서가 정해지면 $P(x_1)$을 샘플링 한 뒤, 귀납적으로 $P(x_{n} \mid x_{n-1})$을 샘플링해 최종 아웃풋을 만들어냄

  - 어떤 그래프에서는 하나 이상의 순서가 가능한데, ancestral sampling는 모두에 적용될 수 있음

  - 계산이 간단하기 때문에 빠르고 편리하게 샘플링을 수행할 수 있음
- Ancestral sampling의 한 가지 단점은 directed graphical model에만 적용될 수 있다는 것임

  - Undirected model을 directed model로 바꿔서 사용할수도 있지만, 일반적으로는 매우 복잡한 그래프가 되어 다루기 힘든 문제가 됨
  - Directed model로 변환하지 않는다면, cyclical dependency를 해결해야 함
  
    - 모든 변수들이 모든 다른 변수들과 상호작용하여, 샘플링을 위한 분명한 시작점이 없게됨
  - 이와 같이 undirected graph를 이용한 샘플링은 일반적으로 소모적인 작업인데, Chapter 17에서 더 자세히 다룰 예정임



## 16.4 Advantages of Structured Modeling

전술하였듯, probabilistic model을 사용할 때 얻을 수 있는 가장 큰 장점은 학습 및 추론 비용의 절약이다. directed model의 경우 sampling 속도도 빨라진다. graphical model은 edge들을 제거하는 형태로 정보를 부호화한다. 두 node가 edge로 연결되지 않는다는 것은 둘 사이의 직접 상호작용을 모형화할 필요가 없다는 것이다.

또 다른 장점으로, 학습으로 얻은 지식의 표현(weight)과 기존 지식에 기초한 추론으로 얻은 지식의 표현(edge-cutting)을 명시적으로 분리할 수 있다는 것이다.

## 16.5 Learning about Dependencies

좋은 generative model은 visible(관측 가능한) variable v에 관한 분포를 정확하게 포착해야 한다. 일반적으로 그런 v의 다양한 성분들 사이의 의존성이 존재하는데, 이 의존성을 모델에 포함시키는 가장 흔한 방식이 hidden latent variable h를 도입하는 것이다. h를 도입한 모형은 $v_i$와 $v_j$ 사이의 간접적인 종속 관계를 $v_i$→h와 h→$v_j$와 같은 직접적인 종속 관계로 분해해 포착할 수 있다.

직접 연결된 visible variable 사이의 의존성을 model에 포함시키려는 경우, 모든 변수를 all-to-all로 연결하는 것은 너무 코스트가 크므로 서로 밀접하게 연관된 edge들만 연결되도록 해야한다. 이 때 선험적인 지식이 필요할 것이다. 이 문제만 다루는 structure learning이라는 ml 분야가 있다.

## 16.6  Inference, Approximate Inference

Probabilistic model의 주 용도는 variable 사이의 관계를 알아내는 것이다.

latent variable model에서는 observed variable v를 서술하는 feature  $E[h\mid v]$를 추출해야 할 때가 있다. 일반적으로 이런 경우 MLE를 계산하게 되며, 결과적으로 $p(h\mid v)$를 계산해야 한다. 다시 말해 어떤 변수들의 값이 주어질 때 다른 변수들의 값이나 그에 대한 확률분포를 예측하는 추론 문제라고 할 수 있다. 허나, 심층 모형에서 이런 추론 문제는 (analytic하게?) 처리 불가능이다. 그리하여 approximate inference(근사추론) 기법들이 나왔다. 이런 추론을 variational inference(변분추론)이라고도 부른다.

variation inference에서는 진짜 분포 $p(h\mid v)$와 새로 도입하는 분포 $q(h\mid v)$가 최대한 가까워지도록  $q(h\mid v)$를 학습한다.

## 16.7 The Deep Learning Approach to Structured Probabilistic Models

DNN에 쓰이는 probabilistic model에서는 주어진 DNN의 깊이를 계산 그래프가 아니라 그래프 모형을 기준으로 정의한다. latent variable $h_i$와 어떤 visible variable 사이의 최단 경로의 길이가 j라고 할 때 j를 $h_i$의 깊이로 간주할 수 있다.

전통적인 그래프 모형의 경우 변수들이 대부분 가끔은 관측되는 변수들로 구성되어 있고, 각각의 변수들에 의미를 부여하나, DNN에서는 latent variable들이 거의 항상 존재하며 구체적인 의미를 부여하지 않는 경우가 많다. 예를 들어, 전통적인 그래프 모형에서는 latent variable의 구체적인 의미-지능, 환자 증상 등-을 미리 연구자가 정해둔다고 생각하면 된다.

### 16.7.1 Example: Restricted Boltzmann Machine

graphical model을 깊은 학습에 사용하는 예로 RBM을 들 수 있다. RBM은 generative model로 데이터들의 latent factor들을 확률적인 방법으로 얻어낼 수 있는 모델이다.

![_config.yml]({{ site.baseurl }}/assets/ch16/Untitled.png)

RBM은 binary visible unit과 binary hidden unit의 두 층으로 구성된 energy based Model이다. 이 모형의 energy function은 아래와 같다.

$$E(v, h) = - b^Tv - c^Th - v^T Wh$$

![_config.yml]({{ site.baseurl }}/assets/ch16/Untitled1.png)

b, c, W는 제약이 없는 학습 가능한 real number parameter들이다. h layer와 v layer의 관계는 W로 서술된다.

![_config.yml]({{ site.baseurl }}/assets/ch16/Untitled2.png)

hidden layer h 사이, 그리고 visible layer v 사이에는 직접적인 상관관계가 존재하지 않는다고 가정한다.  이런 제한으로부터 아래와 같은 성질이 나온다.

![_config.yml]({{ site.baseurl }}/assets/ch16/Untitled3.png)

이는 아래와 같이 계산되고,

![_config.yml]({{ site.baseurl }}/assets/ch16/Untitled4.png)

따라서 $P(h_i =1 \mid v)$는 아래와 같이 계산될 수 있다.

![_config.yml]({{ site.baseurl }}/assets/ch16/Untitled5.png)

에너지함수는 매개변수들의 선형 조합이므로 미분을 구하기 쉽다. 예를 들어

![_config.yml]({{ site.baseurl }}/assets/ch16/Untitled6.png)

모형을 훈련하면 v에 대한 expression h가 유도된다.

![_config.yml]({{ site.baseurl }}/assets/ch16/Untitled7.png)

RBM은 graph 모형에 대한 DNN의 접근방식을 잘 보여준다.