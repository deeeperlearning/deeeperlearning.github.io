## 18.7 Estimating the Partition Function

- 지금까지 18장에서는 undirected model을 사용하되 partition function 계산을 피하는 방법들을 소개했다. 하지만 partition function을 꼭 계산해야 하는 경우도 있다. 이번 섹션에서는 partition function을 꼭 계산해야 하는 경우 근사하는 방법을 소개한다.
  - 예를들어 모델 $p_A(\vec{x}_A ; \vec{\theta}_{A}) = \frac{1}{Z_A} \tilde{p}_A(\vec{x}_A ; \vec{\theta}_{A})$와 $p_B(\vec{x}_B ; \vec{\theta}_{B}) = \frac{1}{Z_B} \tilde{p}_B(\vec{x}_B ; \vec{\theta}_{B})$중 주어진 데이터 $\{\vec{x}^{(1)}, \vec{x}^{(2)}, ...\}$에 대해 더 잘 맞는 모델을 선택해야 한다고 생각해 보자.
  - 가장 쉬운 방법은 주어진 데이터에 대해 더 높은 likelihood를 가지는 모델을 선택하는 것이다.
  - 이 경우 partition function을 계산하지 않고는 likelihood를 구할 수 없다.
- Partition function을 근사하는 가장 간단한 방법은 importance sampling같은 Monte Carlo method를 사용하는 것이다. 분포 $\tilde{p}_0(x)$에 대해 $Z_0$를 알고있고 $\tilde{p}_1(x)$에 대해 $Z_1$를 계산하고 싶은 경우 다음과 같이 importance sampling을 통해 $Z_1$의 근사값 $\hat{Z}_1$을 계산할 수 있다.
$$
Z_1
= \int \tilde{p}_1(x) dx \\
= \int \frac{p_0(x)}{p_0(x)} \tilde{p}_1(x) dx \\
= Z_0 \int p_0(x) \frac{\tilde{p}_1(x)}{\tilde{p}_0(x)} dx \\
$$

$$
\hat{Z}_1 = \frac{Z_0}{K} \sum_{k=1}^K \frac{\tilde{p}_1(x^{(k)})}{\tilde{p}_0(x^{(k)})} \\
\text{s.t. : }x^{(k)} \sim p_0
$$

- 하지만 importance sampling을 하기 위해 선택한 $p_0$가 $p_1$과 굉장히 다른 분포라면 ($p_1$의 값이 큰 점에서 $p_1$의 값이 굉장히 작음) $\frac{\tilde{p}_1(x^{(k)})}{\tilde{p}_0(x^{(k)})}$ 부분이 굉장히 작아진다. 즉, importance sample이 적어지기 때문에 추정값이 부정확해진다.
  - 또한 $\hat{Z}_1$의 분산을 다음과 같이 쓸 수 있는데, 분산이 굉장히 커진다.

$$
\hat{\text{Var}} (\hat{Z}_1) = \frac{Z_0}{K^2} \sum_{k=1} ^ K (\frac{\tilde{p}_1(x^{(k)})}{\tilde{p}_0(x^{(k)})} - \hat{Z}_1)^2
$$

- 일반적으로 좋은 $p_0$를 선택하는 일은 굉장히 어렵다. 이번 장에서는 좋지 않은 $p_0$를 선택하더라도 importance sampling의 약점을 보완할 수 있는 방법에 대해 설명한다.

### 18.7.1 Annealed Importance Sampling

**Concept of Annealed Importance Sampling**

- Annealed Importance Sampling (AIS)는 분포의 sequence $p_0=p_{\eta_0}, p_{\eta_1}, ..., p_{\eta_n}=p_1$을 이용해 $p_0$와 $p_1$이 다른 경우의 문제를 해결한다.
- 즉, 이미 알고있는 간단한 확률분포($p_0$)에서 출발해 점차적으로 $p_1$에 가까워져 간다.

$$
\frac{Z_1}{Z_0}
= \frac{Z_1}{Z_0}\frac{Z_{\eta_1}}{Z_{\eta_1}} ... \frac{Z_{\eta_{n-1}}}{Z_{\eta_{n-1}}} \\
= \frac{Z_{\eta_{1}}}{Z_0}\frac{Z_{\eta_2}}{Z_{\eta_1}} ... \frac{Z_{\eta_{n-1}}}{Z_{\eta_{n-2}}}\frac{Z_{\eta_1}}{Z_{\eta_{n-1}}} \\
= \prod_{j=0}^{n-1} \frac{Z_{\eta_{j+1}}}{Z_{\eta_{j}}}
$$

- 중간에 등장하는 분포들 $p_{\eta_1}, ..., p_{\eta_{n-1}}$은 잘 선택해야 한다.
  - 문제를 푸는 분야의 지식을 이용해 디자인하는 방법 등.

**General-purpose Intermediate Distributions**

- 도메인 지식에 상관없이 intermediate distribution을 아래처럼 선택하는 방법도 있다.

$$
p_{\eta_j} \propto p_1^{\eta_j} p_0^{1-\eta_j}
$$

- 이후 Markov chain transition functions $T$를 아래처럼 정의하면 $x'$ 분포를 $x$로 변환할 수 있다.

$$
p_{\eta_j}(\vec{x}) = \int p_{\eta_j}(\vec{x}') T(\vec{x} \vert \vec{x}') d\vec{x}'
$$

- 따라서 AIS을 아래처럼 진행할 수 있다. $k=1, ..., K$에 대하여,
  - Sample $\vec{x}_{\eta_1}^{(k)} \sim p_0(\vec{x})$
  - Sample $\vec{x}_{\eta_2}^{(k)} \sim T(\vec{x}_{\eta_2} \vert \vec{x}_{\eta_1})$
  - ...
  - Sample $\vec{x}_{\eta_n}^{(k)} \sim T(\vec{x}_{\eta_n} \vert \vec{x}_{\eta_{n-1}})$

$$
w^{(k)}
=
\frac{\tilde{p}_{\eta_1}(\vec{x}_{\eta_1}^{(k)})}{\tilde{p}_0(\vec{x}_{\eta_1}^{(k)})}
\frac{\tilde{p}_{\eta_2}(\vec{x}_{\eta_2}^{(k)})}{\tilde{p}_{\eta_1}(\vec{x}_{\eta_2}^{(k)})}
...
\frac{\tilde{p}_1(\vec{x}_1^{(k)})}{\tilde{p}_{\eta_{n-1}}(\vec{x}_{\eta_n}^{(k)})}
$$

$$
\frac{Z_1}{Z_0} \approx \frac{1}{K} \sum_{k=1}^K w^{(k)}
$$


### 18.7.2 Bridge Sampling

- AIS처럼 여러 단계를 거치는 대신 bridge distribution $p_*$를 잘 선택하여 importance sampling의 오차를 줄이는 방법도 있다.

$$
\frac{Z_1}{Z_0}
\approx
\sum_{k=1}^K \frac{\tilde{p}_*(\vec{x}_0^{(k)})}{\tilde{p}_0(\vec{x}_0^{(k)})}
/
\sum_{k=1}^K \frac{\tilde{p}_*(\vec{x}_0^{(k)})}{\tilde{p}_1\vec{x}_1^{(k)})}
$$

- 이 방법은 $p_*$가 $p_0$, $p_1$과 모두 큰 overlap을 가지게 설계할 수 있다면 잘 동작한다.

**Optimal Bridge Distribution**

$$
p_*^{(\text{opt})}
\propto
\frac{\tilde{p}_0(\vec{x})\tilde{p}_1(\vec{x})}{r\tilde{p}_0(\vec{x}) + \tilde{p}_1(\vec{x})},
\qquad
r = \frac{Z_1}{Z_0}
$$

- Optimal bridge distribution은 위처럼 정의된다.
- 언뜻 봐서는 optimal distribution을 계산하는 과정에 계산하려 하는 값 $r$이 포함되어 불가능한 방법처럼 보일 수 있다. 하지만 iterative하게 $r$을 업데이트 하면 계산이 가능하다.

**Linked Importance Sampling**

- AIS와 bridge sampling 각각의 단점이 있다.
  - $D_{\text{KL}}(p_0 \vert\vert p_1)$이 크지 않으면 AIS보다 bridge sampling이 partition function을 더 효과적으로 추정.
  - $D_{\text{KL}}(p_0 \vert\vert p_1)$이 굉장히 큰 경우 bridge sampling은 $p_0$와 $p_1$에 모두 overlap이 있는 분포를 만들기 어렵지만 AIS는 이를 더 작은 단계로 나누어 처리하므로 두 분포를 연결하는 것이 가능.

**Estimating the Partition Function while Training**

- Partition function 추정에 일반적으로 AIS가 사용되지만, 계산 비용이 크기 때문에 훈련 과정에서 사용하기는 어렵다.
- 이를 해결하기 위해 훈련 과정에서 partition function을 추정할 수 있는 여러 방법이 연구되었다.
- 예를들어 [Desjardins et al. (2011)]에서는 적은 수의 중계 분포를 사용하는 AIS와 병렬 tempering을(여러 AIS sequence를 병렬적으로 계산하는 방법인 듯?) 조합하였다.