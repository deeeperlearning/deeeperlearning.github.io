# 제목 없음

# 3.11 Baye's Rule

---

- 확률변수 $\mathsf{x}$와 $\mathsf{y}$에 대하여 $P(\mathsf{y}\mid \mathsf{x})$와 $P(\mathsf{x})$를 알 때, $P(\mathsf{x}\mid \mathsf{y})$를 구하는 공식.

    $$P(\mathsf{x} \mid \mathsf{y}) = \frac{P(\mathsf{x})P(\mathsf{y}\mid \mathsf{x})}{P(\mathsf{y})}$$

    $P(\mathsf{y})$는 $P(\mathsf{y}) = \sum_{\mathsf{x}} P(\mathsf{y} \mid \mathsf{x})P(\mathsf{x})$로 부터 구할 수 있다.

- 조건부 확률의 정의 $P(\mathsf{y} \mid \mathsf{x}) = P(\mathsf{x,y})/P(\mathsf{x})\ ,\ P(\mathsf{x} \mid \mathsf{y}) = P(\mathsf{x,y})/P(\mathsf{y})$로 부터 쉽게 유도 가능하다. ($P(\mathsf{x,y})$ 를 소거해 주면 됨)

# 3.12 Technical Details of Continuous Variables

---

- Measure zero

    측정하고 있는 공간에서 어떠한 부피도 차지하지 않을 때 사용하는 표현.

    Ex) 2차원 공간에 놓인 도형의 면적을 측정할 때, 점 또는 선과 같이 측정 값이 0 일 경우 measure zero.

- Almost everywhere

    Measure zero들의 집합을 제외한 모든 공간에서 어떤 조건이 만족할 경우, 그 조건은 almost everywhere 에서 만족한다고 한다.

- 두 확률 변수 $\mathsf{x}$ 와 $\mathsf{y}$ 가 $\mathsf{y} = g(\mathsf{x})$ ($g$ : invertible, continuous, differentiable)를 만족할 때, $\mathsf{x}$ 와 $\mathsf{y}$의 확률밀도는 $\left|p_x(x)dx\right|=\left|p_y(g(x))dy\right|$를 만족한다. 즉,

    $$P_x(x) = P_y(g(x))\left| \frac{\partial g(x)}{\partial x}\right|$$

    $g$에 대한 편미분은 공간의 왜곡을 표현.

# 3.13 Information Theory

---

딥러닝에서 정보이론을 사용하는 이유와 정보이론의 기본적인 개념을 소개한다.

- 정보이론을 사용하는 이유.
    - 앞으로 자주 사용하게 될 확률분포를 characterize 하기위해.
    - 두 확률분포 사이의 유사도를 측정하기 위해.

- Self - information

    $$I(x) = -\log P(x)$$

    이 책에선 기본적으로 자연로그를 사용하므로 $I(x)$의 단위는 nats이다. (base-2 로그를 사용할 경우에는 bits)

- Shannon entropy

    $$H(\mathsf{x}) = \mathbb{E}_{\mathsf{x} \sim P}\left[I(x)\right] = -\mathbb{E}_{\mathsf{x} \sim P}\left[\log P(x)\right]$$

    전체 확률 분포에 대한 불확실성을 나타낸다. 0과 1의 값을 가지는 이항 확률 변수에 대한 Shannon entropy를 보면 이해가 빠르다.

    ![_config.yml]({{ site.baseurl }}/assets/ch3/shannon_entropy.png)

    (x 축은 1을 가질 확률)

- Kullback - Leibler divergence

    $$D_{KL}(P \|Q) = \mathbb{E}_{\mathsf{x}\sim P}\left[\log\frac{P(x)}{Q(x)}\right] = \mathbb{E}_{\mathsf{x}\sim P}\left[\log P(x) - \log Q(x) \right]$$

    - 두 확률분포 함수의 비 유사도(dissimilarity)를 나타내며, 항상 0보다 크거나 같은 값을 가진다. 0에 가까울수록 두 확률분포는 유사한 형태를 띈다.
    - 비대칭적인 성질($D_{KL}(P\|Q) \ne D_{KL}(Q\|P)$) 때문에 true distance measure 라고 할 수 없다.

- Cross entropy

    $$H(P,Q) = -\mathbb{E}_{\mathsf{x}\sim P}\left[\log Q(x)\right] = H(P) + D_{KL}(P\|Q)$$

    $H(P)$는 확률분포 $Q$에 의존하지 않으므로, 확률분포 $P$에 대해 cross entropy를 최소화 하는 것은 $D_{KL}$를 최소화 하는 것과 동일하다.

# 3.14 Structured Probabilistic Models

---

어떤 확률분포가 여러개의 무작위 변수를 포함할 때, 이 확률분포를 여러 factor로 나누어 표현하는 방법.

- Directed model

    directed edge는 두 확률 변수 사이의 의존 관계를 나타낸다. ex) $b$가 $a$에 의존 : $a \longrightarrow b$

    ![_config.yml]({{ site.baseurl }}/assets/ch3/directed_graph.png)

    $$P(a,b,c,d,e) = P(a)P(b\mid a)P(c\mid a,b)P(d\mid b)P(e\mid e)$$

- Undirected model

    ![_config.yml]({{ site.baseurl }}/assets/ch3/undirected_graph.png)

    $$P(a,b,c,d,e) = \frac{1}{Z}\phi^{(1)}(a,b,c)\phi^{(2)}(b,d)\phi^{(3)}(c,e)$$

    graph에서 모두 연결된 node들은 하나의 clique를 형성하며 각각의 clique는 하나의 factor $\phi$와 연관된다. 또한 factor는 확률분포일 필요는 없으며 일반적인 함수일 수 있다. 따라서 $Z$를 통해 normalize 과정이 필요하다.