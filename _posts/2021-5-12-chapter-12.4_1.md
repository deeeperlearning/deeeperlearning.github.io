## 12.4 Natural Language Processing

- Natural Language Processing (NLP): 인간의 언어를 컴퓨터가 이용할 수 있도록 하는 분야

  - 예시) 기계 번역 - 특정 언어로 쓰인 문장을 입력시키면, 다른 언어로 번역하여 배출

  - 일반적인 신경망 기술을 이용해 다양한 기능이 성공적으로 구현되어 옴

  - 하지만, 높은 성능을 위해서는 일부 domain-specific 전략들이 중요함 (예-순차적 데이터의 처리 방식)

  - 일반적으로 개별 글자나 byte의 배열이 아닌, 단어의 배열로서 데이터를 다룸

  - 모든 단어의 조합은 굉장히 방대하기 때문에, 단어 기반 모델은 굉장히 차원이 높고 sparse한 discrete space에서 작동해야 함
  


### 12.4.1 n-grams

- (n-1)번째까지의 토큰(단어 등)으로부터 n번째 토큰의 조건부 확률을 계산하는 모델


![_config.yml]({{ site.baseurl }}/assets/ch12/Eq12_5.PNG)


- Maximum likelihood를 계산하기 위해서, 학습 데이터에서 각각의 가능한 n gram이 몇 번이나 등장하는지를 세기만 하면 되기 때문에, 굉장히 직관적인 모델임

- 따라서 80~90년대에 statistical language modeling 분야에서 핵심적인 모델로 사용되어 옴

- 작은 n에 대해서는 고유의 명칭이 있음 - n=1: unigram, n=2: bigram, n=3: trigram, ...

- 일반적으로 n-gram과 n-1 gram을 함께 학습시켜서, 두 개의 저장된 확률을 이용해 조건부 확률을 계산하기 용이하게 함


![_config.yml]({{ site.baseurl }}/assets/ch12/Eq12_6.PNG)


- 예시) "THE DOG RAN AWAY"를 처리할 때

  - P(AWAY | DOG RAN)의 정보가 있다면, P3(THE DOG RAN)으로 부터 마지막 단어를 예상할 수 있음


![_config.yml]({{ site.baseurl }}/assets/ch12/Eq12_7.PNG)


- 이 모델의 가장 큰 한계는, 대부분의 단어 조합이 training data에 존재하지 않아 확률이 0이라는 것임

  - Pn-1 = 0이면, Pn이 정의 될 수 없음
  
  - 확률에 기본적으로 작은 값을 더해서 사용하거나 (smoothing), 높은 order와 낮은 order의 n gram을 혼용해서 개선해 왔음



### 12.4.2 Neural Language Models

### 12.4.3 High-Dimensional Outputs

#### 12.4.3.1 Use of a Short List

#### 12.4.3.2 Hierarchical Softmax



![_config.yml]({{ site.baseurl }}/assets/ch12/Fig12_3.PNG)
