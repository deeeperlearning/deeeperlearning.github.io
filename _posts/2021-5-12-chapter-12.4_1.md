## 12.4 Natural Language Processing

- Natural Language Processing (NLP): 인간의 언어를 컴퓨터가 이용할 수 있도록 하는 분야

  - 예시) 기계 번역 - 특정 언어로 쓰인 문장을 입력시키면, 다른 언어로 번역하여 배출

  - 일반적인 신경망 기술을 이용해 다양한 기능이 성공적으로 구현되어 옴

  - 하지만, 높은 성능을 위해서는 일부 domain-specific 전략들이 중요함 (예-순차적 데이터의 처리 방식)

  - 일반적으로 개별 글자나 byte의 배열이 아닌, 단어의 배열로서 데이터를 다룸

  - 모든 단어의 조합은 굉장히 방대하기 때문에, 단어 기반 모델은 굉장히 차원이 높고 sparse한 discrete space에서 작동해야 함
  


### 12.4.1 n-grams

- ($n$-1)번째까지의 토큰(단어 등)으로부터 $n$번째 토큰의 조건부 확률을 계산하는 모델


![_config.yml]({{ site.baseurl }}/assets/ch12/Eq12_5.PNG)


- Maximum likelihood를 계산하기 위해서, 학습 데이터에서 각각의 가능한 $n$ gram이 몇 번이나 등장하는지를 세기만 하면 되기 때문에, 굉장히 직관적인 모델임

- 따라서 80~90년대에 statistical language modeling 분야에서 핵심적인 모델로 사용되어 옴

- 작은 $n$에 대해서는 고유의 명칭이 있음 - $n$=1: unigram, $n$=2: bigram, $n$=3: trigram, ...

- 일반적으로 $n$-gram과 $n$-1 gram을 함께 학습시켜서, 두 개의 저장된 확률을 이용해 조건부 확률을 계산하기 용이하게 함


![_config.yml]({{ site.baseurl }}/assets/ch12/Eq12_6.PNG)


- 예시) "THE DOG RAN AWAY"를 처리할 때

  - $P$(AWAY | DOG RAN)의 정보가 있다면,
    $P_3$(THE DOG RAN)으로 부터 마지막 단어를 예상할 수 있음


![_config.yml]({{ site.baseurl }}/assets/ch12/Eq12_7.PNG)


- 이 모델의 가장 큰 한계는, 차원의 저주에 의해 대부분의 단어 조합이 training data에 존재하지 않아 확률이 0이라는 것임

  - $P_{n-1} = 0 $이면, $P_n$이 정의 될 수 없음
  
  - 확률에 기본적으로 작은 값을 더해서 사용하거나 (smoothing), 높은 order와 낮은 order의 $n$ gram을 혼용해서 개선해 왔음



### 12.4.2 Neural Language Models

- Neural Language models (NLM): 단어들의 distributed representation을 이용함으로써 차원의 저주를 해결하고자 함

- $n$-gram models과 달리, 두 단어가 서로 다른 단어임은 인식한채, 동시에 비슷한 단어임을 인식할 수 있음

- Distributed representation은 모델로 하여금 공유하는 특징이 있는 단어들을 비슷하게 인식하도록 함

  - 예) '개'와 '고양이'는 공통점이 많으므로, 문장에서 '개'가 포함된 자리에 '고양이'도 올 수 있음을 예상함

  - 한 문장이 있더라도 정보를 바꿔가며 지수함수적으로 많은 관련 문장들을 만드는 방식으로 차원의 저주를 해소함

- 이 방식을 word embedding이라고도 함 (마치 CNN의 hidden layer에 의한 image embedding처럼)

  - 어휘 수 만큼의 차원에서, 모든 단어들은 한 one-hot 벡터에 대응되어, 모든 단어 사이의 Euclidean 거리는 $\sqrt{2}$ 임

  - 비슷한 문맥에서 자주 등장하거나, 비슷한 특징이 있는 단어들은 서로 가깝도록 embedded됨


![_config.yml]({{ site.baseurl }}/assets/ch12/Fig12_3.PNG)



### 12.4.3 High-Dimensional Outputs

- 많은 언어 인식 앱에서, 글자보다는 단어를 단위로 아웃풋을 생성하고자 함

- 하지만 어휘 수가 많을 때에는, 가능한 경우의 수도 많아져 연산 양이 기하급수적으로 많아짐

- 일차적인 해결 방법으로는 hidden representation을 affine transformation 시켜서 output space을 얻은 뒤, softmax 함수를 적용하면 됨

  - 하지만, 이 affine transformation의 가중치가 어휘 수 만큼의 차원을 가져야 하고, softmax 계산이 모든 아웃풋에 적용되어야 하므로 역시 연산 양은 방대함



#### 12.4.3.1 Use of a Short List

- 초기에는 어휘 수를 1~2만으로 제한하는 방식으로 연산 양 문제를 해결함 (Bengio et al., 2001, 2003)

- 이후 어휘 집합 $\mathbb{V}$을 자주 사용되는 단어의 집합인 shortlist $\mathbb{L}$과 나머지 드문 단어의 집합인 tail $\mathbb{T}$로 나누는 시도가 이루어짐

- 특정 context $C$ 다음의 단어가 드문 단어 그룹 $\mathbb{T}$에 속할 확률은 $P(i\in\mathbb{T}\mid C)$ 임

- 이 때, 해당 단어가 $y$ 일 확률은 아래와 같음

![_config.yml]({{ site.baseurl }}/assets/ch12/Eq12_10.PNG)

  - $P(y=i\mid C,i\in\mathbb{L})$ 은 NLM을, 보다 간단한 $P(y=i\mid C,i\in\mathbb{T})$ 은 n-gram 모델을 이용해서 계산함

- 가장 큰 단점은, NLM을 이용한 일반화에서의 어드밴티지가 매우 활용 빈도가 높아, 곧 의미가 별로 없는 단어에만 적용될 수 있다는 것임

- 따라서 고차원의 아웃풋에 적용할 수 있는 후속 방법들이 등장해 옴



#### 12.4.3.2 Hierarchical Softmax

- 어휘 $\mathbb{V}$ 각각에 대한 확률을 계층적으로 분해하여, 연산 양을 $\mid\mathbb{T}\mid$ 에서 $log \mid\mathbb{T}\mid$ 로 줄일 수 있음

  - 단어의 카테고리를 만들고, 이것의 카테고리를 만들고, 이것의 카테고리를 만들고... 의 방식

![_config.yml]({{ site.baseurl }}/assets/ch12/Fig12_4.PNG)

- 트리의 구조를 최적화하여 연산 양을 최소화하는 것이 이론적으로는 가능하긴 한데, 현실적으로는 힘듬

  - 정보 이론에 기반하여, 최적화된 binary code를 고름

  - 단어당 bit 수가 log(해당 단어의 빈도)와 같도록 트리를 설계하면 됨

  - 하지만 현실적으로는, 아웃풋 계산은 전체 NLM에서 극히 일부분이기 때문에, 줄여봤자 큰 의미가 없음

  - 보다 중요한 과제는 어떻게 일반화된 방법으로 단어의 클래스와 계층을 정하냐는 것임

- 대표적인 장점은 training time과 test time 모두를 줄이는데 효율적임

- 대표적인 단점은 다음에 소개될 sampling-based 방법보다 성능이 낮다는 것임 (아마도 단어 클래스가 제대로 선택되지 않아서)
