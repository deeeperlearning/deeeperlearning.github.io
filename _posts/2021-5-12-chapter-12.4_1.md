## 12.4 Natural Language Processing

- Natural Language Processing (NLP): 인간의 언어를 컴퓨터가 이용할 수 있도록 하는 분야

  - 예시) 기계 번역 - 특정 언어로 쓰인 문장을 입력시키면, 다른 언어로 번역하여 배출

  - 일반적인 신경망 기술을 이용해 다양한 기능이 성공적으로 구현되어 옴

  - 하지만, 높은 성능을 위해서는 일부 domain-specific 전략들이 중요함 (예-순차적 데이터의 처리 방식)

  - 일반적으로 개별 글자나 byte의 배열이 아닌, 단어의 배열로서 데이터를 다룸

  - 모든 단어의 조합은 굉장히 방대하기 때문에, 단어 기반 모델은 굉장히 차원이 높고 sparse한 discrete space에서 작동해야 함
  


### 12.4.1 n-grams

- (n-1)번째까지의 토큰(단어 등)으로부터 n번째 토큰의 조건부 확률을 계산하는 모델


![_config.yml]({{ site.baseurl }}/assets/ch12/Eq12_5.PNG)


- Maximum likelihood를 계산하기 위해서, 학습 데이터에서 각각의 가능한 n gram이 몇 번이나 등장하는지를 세기만 하면 되기 때문에, 굉장히 직관적인 모델임

- 따라서 80~90년대에 statistical language modeling 분야에서 핵심적인 모델로 사용되어 옴

- 작은 n에 대해서는 고유의 명칭이 있음 - n=1: unigram, n=2: bigram, n=3: trigram, ...

- 일반적으로 n-gram과 n-1 gram을 함께 학습시켜서, 두 개의 저장된 확률을 이용해 조건부 확률을 계산하기 용이하게 함


![_config.yml]({{ site.baseurl }}/assets/ch12/Eq12_6.PNG)


- 예시) "THE DOG RAN AWAY"를 처리할 때

  - P(AWAY | DOG RAN)의 정보가 있다면, P3(THE DOG RAN)으로 부터 마지막 단어를 예상할 수 있음


![_config.yml]({{ site.baseurl }}/assets/ch12/Eq12_7.PNG)


- 이 모델의 가장 큰 한계는, 차원의 저주에 의해 대부분의 단어 조합이 training data에 존재하지 않아 확률이 0이라는 것임

  - Pn-1 = 0이면, Pn이 정의 될 수 없음
  
  - 확률에 기본적으로 작은 값을 더해서 사용하거나 (smoothing), 높은 order와 낮은 order의 n gram을 혼용해서 개선해 왔음



### 12.4.2 Neural Language Models

- Neural Language models (NLM): 단어들의 distributed representation을 이용함으로써 차원의 저주를 해결하고자 함

- n-gram models과 달리, 두 단어가 서로 다른 단어임은 인식한채, 동시에 비슷한 단어임을 인식할 수 있음

- Distributed representation은 모델로 하여금 공유하는 특징이 있는 단어들을 비슷하게 인식하도록 함

  - 예) '개'와 '고양이'는 공통점이 많으므로, 문장에서 '개'가 포함된 자리에 '고양이'도 올 수 있음을 예상함

  - 한 문장이 있더라도 정보를 바꿔가며 지수함수적으로 많은 관련 문장들을 만드는 방식으로 차원의 저주를 해소함

- 이 방식을 word embedding이라고도 함 (마치 CNN의 hidden layer에 의한 image embedding처럼)

  - 어휘 수 만큼의 차원에서, 모든 단어들은 한 one-hot 벡터에 대응되어, 모든 단어 사이의 Euclidean 거리는 $\sqrt{2}$ 임

  - 비슷한 문맥에서 자주 등장하거나, 비슷한 특징이 있는 단어들은 서로 가깝도록 embedded됨


![_config.yml]({{ site.baseurl }}/assets/ch12/Fig12_3.PNG)



### 12.4.3 High-Dimensional Outputs

- 많은 언어 인식 앱에서, 글자보다는 단어를 단위로 아웃풋을 생성하고자 함

- 하지만 어휘 수가 많을 때에는, 가능한 경우의 수도 많아져 연산 양이 기하급수적으로 많아짐

- 일차적인 해결 방법으로는 hidden representation을 affine transformation 시켜서 output space을 얻은 뒤, softmax 함수를 적용하면 됨

  - 하지만, 이 affine transformation의 가중치가 어휘 수 만큼의 차원을 가져야 하고, softmax 계산이 모든 아웃풋에 적용되어야 하므로 역시 연산 양은 방대함



#### 12.4.3.1 Use of a Short List

- 초기에는 어휘 수를 1~2만으로 제한하는 방식으로 연산 양 문제를 해결함 (Bengio et al., 2001, 2003)

- 이후 어휘 집합 $\mathbb{V}$을 자주 사용되는 단어의 집합인 shortlist $\mathbb{L}$과 나머지 드문 단어의 집합인 tail $\mathbb{T}$로 나누는 시도가 이루어짐

- 특정 context $C$ 다음의 단어가 드문 단어 그룹 $\mathbb{T}$에 속할 확률은 $P(i\in\mathbb{T}\mid C)$ 임

- 이 때, 해당 단어가 $y$ 일 확률은 아래와 같음

![_config.yml]({{ site.baseurl }}/assets/ch12/Eq12_10.PNG)

  - $P(y=i\mid C,i\in\mathbb{L})$ 은 NLM을, 보다 간단한 $P(y=i\mid C,i\in\mathbb{T})$ 은 n-gram 모델을 이용해서 계산함

- 가장 큰 단점은, NLM을 이용한 일반화에서의 어드밴티지가 매우 활용 빈도가 높아, 곧 의미가 별로 없는 단어에만 적용될 수 있다는 것임

- 따라서 고차원의 아웃풋에 적용할 수 있는 후속 방법들이 등장해 옴



#### 12.4.3.2 Hierarchical Softmax

- 



