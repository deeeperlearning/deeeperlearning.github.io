## 15.3 Semi-supervised Disentagling of Causal Factors

representation learning에서 중요한 건 "어떤 representation이 좋은 건가?"이다. 서로 다른 feature들이 관측된 자료의 서로 다른 원인들에 대응되면서, 서로 풀어해쳐지는(disentangle) 효과를 내는 표현 방식이 이상적일 것이다. representation learning의 다른 접근으로는, 모델링하기 쉬운 representation을 사용하는 방법이 있다. 예를 들어 성분들이 sparse하거나 서로 독립적인 경우가 있겠다. causal factor들이 숨겨져있어 이를 모형화하기 어려운 경우도 많다. pretraining이 도움이 되는 경우와 그렇지 않은 경우의 예시는 아래와 같다. 

- pretraining이 $p(y\mid x)$에 도움이 되지 않는 경우: $p(x)$가 uniform distribution이고 $E[y\mid x]$를 배워야 하는 경우.
- pretraining이 성공적으로 적용될 수 있는 경우: 아래 그림과 같이 $p(x)$가 일종의 mixture of gaussian이라고 해보자. 이 때 이 mixture 분포가 y의 각 값당 하나의 성분 분포를 혼합한 것이라면, $p(x)$를 적절히 모형화하는 경우 y를 찾기 쉬워져 $p(y\mid x)$가 잘 학습된다.

![1]({{ site.baseurl }}/assets/ch15/Untitled1.png)

다시 말해 $y$가 $x$의 원인중 하나와 밀접하게 연관된다면, $p(x)$와 $p(y\mid x)$는 강한 연관관계를 가질 것이다. 이런 경우 pretraining을 동반하는 semi-supervised learning이 유용하게 먹힐 것이다. 무엇을 부호화할 것인지 결정하는 문제는 semi-supervised learning의 주요 연구 과제중 하나다. 현재 연구되는 주된 전략들은 아래와 같다.

- supervised learning signal과 unsupervised signal을 동시에 사용해 모형이 variation의 가장 의미있는 factor을 포착하도록 하는 것
- 더 큰 unsupervised network structure 사용

unsupervised learning의 다른 전략중 하나는 가장 두드러진 underlying cause의 정의를 수정하는 것이다. 즉 고정된 판정 기준을 사용하는 것이 아니라(MSE같은) 변동하는 기준을 사용하는 것이다. 대표적인 예로 Generative adversarial network가 있다. 한 generative model과 "detective" classifier로 구성되어 있는데, generative model은 classifier가 가짜임을 드러내지 못하도록 훈련되고, classifier은 generative로부터 나오는 input이 real input인지 artificial인지 구분하는 모델을 학습한다.

![2]({{ site.baseurl }}/assets/ch15/Untitled2.png)

![3]({{ site.baseurl }}/assets/ch15/Untitled3.png)

Underlying causal factors를 학습하는 전략의 장점은, $x$가 output이고 $y$가 cause라면, $p(x\mid y)$에 대한 모형이 $P(y)$의 변화에 robust하다는 것이다. 다시 말해 underlying causal factor에 대한 marginal distribution은 변하지만, Casual mechanism은 변하지 않는다.