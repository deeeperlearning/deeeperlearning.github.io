## 15.6 Providing Clues to Discover Underlying Causes

- 단원을 마무리하며, 근복적인 질문을 짚어보자: 무엇이 좋은 representation인가?

  - 15.3 복습: 데이터 다양성의 원천이 되는 요소를 파악하는 representation이 이상적임 (특히 주어진 과제와 관련이 있는)

  - Representation 학습을 이용하는 대부분의 전략들은 이러한 요소들을 찾을 수 있도록 단서를 제공하고자 함

  - 이러한 단서들은 특정 요소들을 다른 요소들과 구분짓도록 도와줌



- 지도 학습에서는 굉장히 강력한 단서가 주어짐: label $y$와 적어도 한 요소는 서로 다른 인풋 $x$

  - 일반적으로 많은 양의 미표시 데이터를 사용하기 위해서, representation 학습은 직접적이지 않더라도 이러한 요소들에 담긴 힌트들을 활용함

  - 물론 완벽한 정규화 전략이란건 없지만, 다양한 과제에 적용할 수 있는 꽤 범용성 있는(fairly generic) 정규화 전략을 찾으려는 시도임



- 이러한 측면에서 기반이 되는 요소를 파악할 수 있도록 다양하게 활용되고 있는 정규화 전략들을 소개함

  - Smoothness
  
    - $f(x+\epsilond) \approx f(x)$를 가정하여 학습 데이터 근처의 인픗 공간까지로 일반화할 수 있도록 함

    - 많은 기계 학습 알고리즘에 적용되지만, 차원의 저주를 해결할 수 는 없음
    
    
  - Linearity
    
    - 어떤 변수 사이의 관계가 선형이라고 가정함으로써, 관찰된 데이터에서 매우 먼 지점까지도 예상 할 수 있음
    
    - Smoothness 가정을 사용하지 않는 많은 기계 학습 알고리즘들이 linearity를 가정함
    
    - 그러나 비상식적인 결과가 나올수도 있으며, 가중치가 큰 경우 함수가 smooth 해지지 않을 수 있음

    
  - Multiple explanatory factors
  
    - 데이터가 여러 요소에 의해 정해진다면, 각각을 파악함으로써 문제를 해결할 수 있음

    - 예) $p(x)$를 예상하기 위해, $p(y \mid x)$를 파악하기
    
    
  - Causal factors: 관측된 데이터 $x$의 원인이 되는 representation $h$를 파악함으로써 문제를 해결할 수 있음

    
  - Depth (or a hierarchical organization of explanatory factors)
    
    - 높은 수준의 개념을 얻어내기까지의 여러 앞먹임 단계를 파악하여 문제를 해결할 수 있음
     
    
  - Shared factors across tasks: 여러 문제를 해결해야 할 때, representation을 공유함으로써 각 문제의 통계적인 파워를 나눌 수 있도록 함
  
    
  - Manifolds: 원래의 공간보다 더 낮은 차원으로 구성된 manifold를 이용해 매우 작은 타겟 공간을 파악함 (예 - 오토인코더)


  - Natural clustering: 일반적인 경우와 달리, 서로 연결되지 않은 manifold들의 데이터가 한 클래스에 포함 될 수도 있기 때문에 이러한 형태의 clustering을 고려함
    
    
  - Temporal and spatial coherence: 대부분의 주요 요소들은 시간, 공간적으로 천천히 변화하거나, 적어도 픽셀 값보다는 이러한 주요 요소들이 예상하기 쉽다는 점을 이용함
    
    
  - Sparsity: 대부분의 feature들은 목표하는 과제에 사용되지 않는데, 이러한 feature들을 무시함으로써 문제가 쉬워지도록 함
   
    
  - Simplicity of Factor Dependencies: 요소들 사이의 관계를 단순화 시킴으로써 효과적인 representation을 얻음 (예 - linearity, 확률 독립성)
