## 15.2 Transfer Learning and Domain Adaptation

Transfer Learning (TL) 과 Domain Adaptation(DA)은 하나의 설정(예를들어 분포 $P_1$)에서 학습한 것을 다른 설정(분포 $P_2$)의 일반화를 개선하기 위해 활용하는 것을 말한다.
이번 절에서는 TL과 DA에 대해 구체적인 구현 방법을 소개하기보다는 추상적인 개념들을 전달한 후 예시를 드는 방식으로 설명할 예정이다.

#### Transfer Learning

- $P_2$의 학습을 위해 학습해야 하는 지식이 $P_1$의 지식과 관련이 있는 경우 사용할 수 있다.
- 특히 $P_2$의 데이터셋이 작고 $P_1$의 데이터셋은 굉장히 크다면 $P_2$에 대한 모델의 일반화 성능에 크게 도움을 줄 수도 있다.
- TL은 이미지 관련 분야에서 활발하게 사용된다. 두 분포에서 풀어야 하는 문제가 다르더라도 (e.g. $P_1$에서는 사진을 개와 고양이로 구분, $P_2$에서는 사진을 호랑이와 사자로 구분) 윤곽선이나 기하도형, 기하학적 변화나 조면 변화의 효과 같은 저수준 개념들을 공유하기 때문에 일반화 개선이 가능하다.

![_config.yml]({{ site.baseurl }}/assets/ch15/15_2_1.png)

- 위 그림처럼 서로 다른 과제들이 모델의 입력 부분이 아닌 출력 부분을 공유해야 할 때도 있다. 이럴 경우 모델의 입력 부분은 여러 갈래로 분리하고 출력만 공유하는 형태도 가능하다.
    - 예를들어 여러 언어의 음성을 인식하여 문장으로 바꾸는 모델이 해당될 수 있다. 일단 음성을 latent space로 인코딩 한 후에는 동일한 매핑을 통해 문장으로 바꿀 수 있지만, 입력에서 latent space로 매핑하는 방법은 언어마다 달라져야 한다.
- TL의 두 극단적인 예로는 one-shot learning과 zero-shot learning이 있다.
    - One-shot learning: label이 있는 데이터를 딱 한 샘플만 사용하는 경우.
        - 예시로 보안을 위해 특정 인물의 얼굴만 인식해야 하는 경우가 있다. 이 경우 첫 단계에서 많은 사람 얼굴 사진으로 모델을 학습한 후 TL 단계에서 특정인의 사진 딱 한장("특정인"이라는 label이 있는 샘플인 셈)만 가지고 모델을 학습할 수 있다.
        - 이런게 가능한 이유는 첫 단계에서 데이터의 배경에 깔린 클래스들을 깔끔하게 나누었기 때문이다. 즉, TL 단계에서 label이 있는 샘플이 하나만 주어지더라도 해당 label이 존재할 수 있는 영억을 모델이 추론할 수 있다.
    - Zero-shot learning: label이 없는 데이터만 사용하는 경우.
        - 예시로 대량의 텍스트만 읽은 후 이미지에서 물체를 인식하는 문제가 있다. 만약 모델이 읽은 텍스트 중 고양이를 충분히 잘 서술한 텍스트가 있다면 (e.g. 다리가 네 개이고 귀가 뾰족하고...) 주어진 이미지에 고양이가 있는지 없는지 판단할 수 있다.
        - 비슷한 현상이 기계 번역에서도 나타난다. 언어 X의 문장을 Y의 문장으로 번역하는 모델이 있다고 하자. 언어 X의 단어 A를 Y의 단어 B로 번역하라는 데이터가 없더라도 모델은 이를 추론할 수 있다.
        - 이러한 학습은 아래 그림처럼 한 modality에서의 변수 $x$를 $h_x$로 매핑하는 방법, 또 다른 modality에서의 변수 $y$를 $h_y$로 매핑하는 방법, $h_x$와 $h_y$의 관계를 모두 배우는 셈이다.

![_config.yml]({{ site.baseurl }}/assets/ch15/15_2_2.png)

#### Domain Adaptation

- DA는 한 설정의 분포를 이용해 다른 설정의 일반화를 개선한다는 점은 같지만, 입력의 분포가 약간 달라진다는 면에서 다른다.
- 예를들어 MNIST 데이터로 숫자 인식을 학습한 모델은(윗줄) MNIST의 배경을 무작위 사진으로 바꾼 입력(아랫줄; MNIST-M)에 대해 숫자를 거의 인식하지 못한다.

![_config.yml]({{ site.baseurl }}/assets/ch15/15_2_3.png)

- DA의 목표는 입력의 분포가 조금 달라지더라도 latent space로 매핑되었을 때 분포가 같아지도록 만드는 것이다.
    - 위의 MNIST에는 label이 있지만 MNIST-M에는 label이 없는 경우 MNIST-M의 숫자를 인식해야 하는 문제를 생각해보자. 학습하려는 모델이 크게 입력을 latent vector로 매핑하는 함수 $h = f(x)$와 latent vector를 label로 매핑하는 분류기 $y = g(h)$로 이루어져 있다고 하자. 이 경우 $f(x)$가 MNIST와 MNIST-M을 같은 latent 분포로 매핑할 수 있다면, 분류기는 MNIST로 학습한 것을 그대로 사용할 수 있다.
    - 이렇게 latent space의 분포를 일치시키는 방법은 여러가지가 있지만 크게 두 가지가 있다.
        - Latent space에서 maximum mean discrepancy(MMD)의 차이를 줄이는 방법: MMD는 두 분포의 거리를 재는 방법 중 하나이다. MNIST를 인코딩 한 경우와 MNIST-M을 인코딩 한 경우의 MMD를 줄이도록 학습하면 latent space에서 분포를 일치시킬 수 있다.
        - GAN을 이용하는 방법: discriminator를 하나 두어 MNIST를 인코딩 한 경우의 latent vector와 MNIST-M을 인코딩 한 경우의 lantet vector를 구분하고 하고, 입력을 latent vector로 인코딩 하는 모듈은 discriminator가 이 둘을 구분하지 못하게 한다면 분포를 일치시킬 수 있다. MMD 이후에 많이 사용되는 방법이다.
