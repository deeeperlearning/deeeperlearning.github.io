이 장에서는 심층 신경망의 형태와 학습 방법에 대하여 다룬다. 먼저 심층 신경망이 무엇인지 잠깐 소개한 후 6.1부터 구체적인 내용을 다룰 예정이다. 우선 심층 순방향 신경망의 형태적인 특징은 다음과 같다.

- 이 책에서 다루는 신경망은 여러 층으로 구성되어 있다. 일반적으로 각 층은 input 벡터를 output 벡터로 mapping하는 함수 정도로 생각하면 된다.
- 각 층을 $f^{(i)}(x)$라고 표현한다면 전체 신경망은 $f(x) = f^{(1)} \circ f^{(2)} \circ ... \circ f^{(i)}(x)$이다.
- 값이 순방향으로만 전파되기 때문에 이러한 이름이 붙었다. 값이 되먹임되는 매커니즘이 추가된 recurrent neural network도 있는데 이는 추후에 등장한다.
- 신경망의 목표는 주어진 데이터를 잘 표현하는 함수를 근사하는 것이다. 예를들어 지도학습의 경우 input $x$, 이에 대응되는 label $y$에 대하여 $y \approx f^*(x)$인 $f^*$를 찾는것이 목표이다.

심층 순방향 신경망에서는 비선형성이라는 요소도 굉장히 중요하다. 기존의 머신러닝 기법들에서도 선형 모델의 한계가 명확하기 때문에 비선형 함수를 최적화하려는 노력이 계속되어 왔다. 이러한 논리는 심층 순방향 신경망에서도 유의하며 이를 위해 각 층의 output에 대하여 비선형함수인 activiation function을 적용한다. 기존의 머신러닝 알고리즘들도 모델에 비선형성을 추가하기 위해 여러 방법들을 사용했는데 신경망에서 activation function을 이용하는 이유와 기존 방법들의 단점은 다음과 같다.

- (기존 모델) RBF 커널에 기초하여 무한차원 커널을 사용하는 방법: 커널의 차원이 충분히 높으면 데이터를 표현하기에 충분한 capacity를 가진다. 하지만 overfitting이 발생할 위험이 크다.
- (기존 모델) 특화된 비선형 함수를 사람이 설계하는 방법: 해당 분야에 대한 지식이 충분하다면 좋은 방법이다. 하지만 좋은 비선형 함수를 설계하기 위해 수많은 연구가 필요하다.
- (심층 순방향 신경망) 비선형 함수 자체를 학습: 신경망의 각 층이 다음 층으로 신호를 보내기 전에 activation function을 적용하는 경우 신경망의 각 층은 비선형함수를 매개변수화하여 표현한다. 즉, 우리는 매개변수를 학습 알고리즘을 통해 최적화하고 비선형함수를 학습할 수 있다. 이러한 방법을 사용하면 신경망의 형태를 조절하여 overfitting을 방지할수도 있고, 신경망의 형태가 적절한 함수족을 표현할 수 있도록 선택하기만 하면 좋은 성능을 낼 가능성이 있는 비선형 함수를 학습할 수 있다.

## 6.1 Example: Learning XOR

이 장에서는 심층 순방향 신경망의 구체적인 형태를 이해하기 위해 간단한 예제를 풀어볼 것이다. 아직 학습 알고리즘을 소개하지 않았기 때문에 XOR 문제에 대하여 신경망의 해를 직접 구해볼 것이다(구한다는 표현이 좀 어색한데, 문제가 너무 간단해서 적절한 매개변수값을 유추할 수 있다).

XOR은 두 이진수 $x_1, x_2$에 대한 연산이다. 두 숫자 중 하나가 1이면 1을 반환하고 그렇지 않으면 0을 반환한다. 즉, 우리가 신경망으로 근사해야 하는 함수는 input이 $(x_1, x_2) = (0, 0) \text{ or } (1, 1)$인 경우 0을, $(x_1, x_2) = (0, 0) \text{ or } (1, 1)$인 경우 1을 반환해야 한다.

$$X = \{[x_1, x_2]^T | x_1 \in \{0, 1\}, x_2 \in \{0, 1\} \}$$

우선 가장 간단한 형태의 선형 모델을 XOR 근사에 사용해보고 선형 모델의 한계를 살펴보자. 사용한 선형 모델과 앞 장에서 소개한 MSE error는 다음과 같다.

$$\vec{x} = [x_1, x_2]^T \\ f(\vec{x};\vec{w}, b) = \vec{x}^T\vec{w} + b \\ J(\vec{w}, b) = \frac{1}{4}\sum_{\vec{x} \in X} (y - f(\vec{x};\vec{w}, b))$$

이전 장에서 MSE error에 대하여 선형 모델의 닫힌 형식의 표준방정식을 유도했는데, 이를 적용하면 $\vec{w} = \vec{0}, b = \frac{1}{2}$이다. 즉, 선형 모델은 모든 input에 대하여 0.5를 출력한다. 이는 선형 모델이 비선형 모델인 XOR를 표현할 수 없기 때문이며 아래의 그림을 보면 선형 모델이 이 문제를 풀지 못한 이유를 알 수 있다.

![_config.yml]({{ site.baseurl }}/assets/ch6/input_space.png)

이번에는 모델에 비선형성을 추가하기 위해 두 층으로 이루어진 선형 신경망을 이용해보자. 사용할 신경망의 계산은 다음과 같이 이루어진다.

$$\vec{h} = f^{(1)}(\vec{x}; W, \vec{c}) = g(W^T\vec{x} + \vec{c}) \\ y' = f^{(2)}(\vec{h}; \vec{w}) = \vec{w}^T\vec{h}$$

식에서 $g$는 벡터의 각 성분별로 적용되는 ReLU 함수이다. 식의 $\vec{h}$는 신경망의 첫번째 층이 출력한 값, $y'$은 두번째 층이 출력한 값이라고 생각하면 된다. 신경망의 매개변수들을 다음과 같이 선택하면 신경망이 XOR 연산을 할 수 있게 된다.

$$W =\left[ \begin{matrix} 1 & 2 \\ 3 & 4 \\ \end{matrix} \right] \\ \vec{c} = [0, -1] \\ \vec{w} = [1, -2]$$

이제 두개의 층으로 이루어진 순방향 신경망이 XOR을 근사할 수 있었던 이유를 살펴보자. 아래는 신경망의 첫번째 층에서 데이터의 분포이다. 비선형 함수를 이용하여 input space가 왜곡되었고 결과적으로 선형 함수인 신경망의 두번째 층으로 근사할 수 있는 분포를 만들어냈다.

![_config.yml]({{ site.baseurl }}/assets/ch6/latent_space.png)

## 6.6 Historical Notes

이 장에서는 지금까지 딥러닝이 발전해 온 과정을 간략하게 소개한다. 역사를 모두 알 필요는 없지만 각 시대별로 어떠한 이유에 의해 모델이 변경되고 발전해왔는지 기억해두면 좋을 것 같다.

- 시대별 큼직한 사건들
    - 17세기: back-propagation의 근간이 되는 chain rule이 발명됨.
    - 1940년대: function approximation 기법들이 사용되었다. 주로 선형 모델들을 이용했는데, XOR문제를 해결하지 못하는 등의 한계로 사장되었다.
    - 1960, 1970년대: 비선형 함수를 표현하기 위해 여러 층으로 이루어진 신경망이 도입되었다. 이러한 신경망의 학습을 위해 back-propagation이 개발되었다.
    - 1990년대 ~ 2006년: 1990년대에 신경망 연구가 굉장히 활발했지만 다른 2006년가지는 다른 머신러닝 기법들이 더 많이 사용됨.
- 최근 딥러닝이 많이 사용되는 이유
    - 사실 신경망 알고리즘에 중요한 기술들은 1980년대 이후로 크게 변한게 없다. 그럼에도 크게 발전한 이유는 크게 두 가지 이유가 있다.
        - 굉장히 큰 데이터셋을 사용할 수 있게 되었다. 이에 따라 모델이 데이터에 대해 일반화 할 수 있는 수준이 올라갔다.
        - 컴퓨터의 연산 능령이 크게 향상되었고 소프트웨어 인프라가 좋아졌다.
    - 알고리즘 자체가 발전한 점도 몇가지 있다.
        - MSE loss 대신 cross-entropy 형태의 loss를 사용하기 시작했고 MLE 관점에서 접근하기 시작했다. 이는 sigmoid나 softmax output을 사용하는 모델들의 성능을 크게 향상시켰다.
        - Sigmoid 형태의 활성화함수를 ReLU로 대체하였다. 이전에는 sigmoid 형태의 활성화함수가 작은 모델에서 더 좋은 성능을 내는 점, 미분 불가능한 함수를 사용하면 안된다는 믿음 때문에 ReLU를 사용하지 않았다.
