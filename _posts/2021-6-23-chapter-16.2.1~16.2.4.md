- 구조화된 확률 모델은 그래프로 나타낼 수 있다.
  - 노드: 확률변수
  - 엣지: 확률변수 사이의 관계
- 확률 모형을 그래프로 나타내는 여러가지 방법이 있다. 이번 절에서는 그 중 많이 사용되는 방법들을 소개한다.

### 16.2.1 Directed Models
- 그래프 모델은 크게 방향성이 있는 경우와 없는 경우로 나눌 수 있다. 이번 절에서는 방향성이 있는 경우의 모델을 다룬다.
  - directed model, belief network, Bayesian network 등으로 불린다.

![_config.yml]({{ site.baseurl }}/assets/ch16/16.2.1_1.png)

- Directed model은 변수 사이의 조건부 확률을 화살표로 나타낸다. 식으로 표현하면 다음과 같다.
  - $p(\vec{x}) = \prod_i (x_i \vert Pa_G(x_i))$
  - $Pa_G(x_i)$는 $x_i$의 부모 노드들을 의미한다.
  - 위의 예시 그림을 식으로 표현하면 $p(t_0, p_1, p_2) = p(t_0)p(t_1 \vert t_0)p(t_2 \vert t_1)$

- directed model을 사용하면 더 적은 매개변수를 이용하여 모델을 표현할 수 있다.
  - 예를들어 위 그림에서 $t_0, t_1, t_2$ 모두 100개의 가능한 값 중 하나를 가지는 불연속 확률변수라고 생각해보자. 만약 $p(t_0, t_1, t_2)$를 확률표로 나타내면 총 $999,999=100\times100\times100 - 1$개의 매개변수가 필요하다(전체의 합이 1이므로 하나는 기록하지 않아도 됨). 하지만 directed model을 이용하여 $p(t_0, t_1, t_2) = p(t_0)p(t_1 \vert t_0)p(t_2 \vert t_1)$로 나타낸다면 $99+2\times(100\times100 - 1)$개의 매개변수로 모델을 표현할 수 있다.
  - 즉, 일반적으로 $k$개의 값 중 하나를 가지는 불연속 확률변수 $n$개를 모델링하기 위해 $O(k^n)$개의 매개변수가 필요하다. 하지만 directed model을 사용한다면 $O(K^m)$ ($m$은 하나의 조건부 확률 표현에 등장하는 최대 변수의 개수)개의 매개변수만 이용하여 표현할 수 있다.

- directed model에 모든 종류의 정보를 녹여낼 수 있지는 않다.
  - 예를들어 위 그림에서 Bob은 완주하는데 항상 10초가 걸린다고 하자. 즉 $t_1 = t_0 + 10$이다. 이는 확률 모델의 파라미터 개수를 줄이는 아주 중요한 정보이지만 directed model 자체에는 반영되지 않는다. 이는 확률표에만 반영된다($p(t_1 \vert t_0)$를 표현하는 확률표에 숫자가 하나만 있으면 됨).


### 16.2.2 Undirected Models
- Undirected model은 상호작용의 방향성이 없는 경우에 유용한 모델이다.
  - Undirected models, Markiv random fields, Markov networks 모두 다 같은 말이다.

![_config.yml]({{ site.baseurl }}/assets/ch16/16.2.2_1.png)

- Undirected model은 변수 사이의 상관관계를 직선으로 표현한다. 식으로는 다음과 같다.
  - $\tilde{p}(\vec{p}) = \prod_{C\in G} \phi(C)$
  - Clique $C$는 그래프 $G$에서 노드들이 모두 서로 연결된 부분집합을 의미.

    (clique 예시 그림)

  - $\phi(C)$는 $C$에 속한 노드들 사이의 상호작용 강도 (affinity)를 의미한다.
    - 이 값은 항상 non-negative이다.
    - 확률값은 아니다.
    - 물리학의 potential에 해당하는 값이다.

- $\phi(C)$를 표현하는 affinity 표는 unnormalized probability distribution이다.
  - 예를들어 $h_y$와 $h_c$를 포함하는 clique의 affinity 표는 다음과 같다.

    |       |$h_y=0$|$h_y=1$|
    |---    |:---:  |:---:  |
    |$h_c=0$|2      |1      |
    |$h_c=1$|1      |10     |


### 16.2.3 The Partition Function
- Undirected model을 표현하는 $\tilde{p}(\vec{x})$는 normalization이 되지 않은 상태이다.
- 이를 확률값으로 변환하기 위해서는 normalize factor가 필요한데 이를 partition function($Z$)이라 부른다.
- 즉, undirected model의 확률 분포는 다음과 같이 쓸 수 있다.
  - $p(\vec{x})=\frac{1}{Z}\tilde{p}(\vec{x})$
  - $Z=\int\tilde{p}(\vec{x})d\vec{x}$

- $Z$는 모든 경우의 수에 대한 합(또는 적분)이다. 일반적으로 계산량이 많아 계산이 불가능하다.
- 딥러닝의 관점에서도 $Z$는 계산이 불가능하다. 18단원에서는 이른 근사하는 방법을 소개한다.

- $Z$가 수학적으로 항상 계산 가능한것은 아니다. Clique potential의 형태에 따라 $Z$가 발산하는 경우도 있다.
  - 예를들어 $\phi(x)=x^2$, $Z=\int x^2 dx$인 경우 $Z$가 발산한다.

- $Z$ 값은 clique potential은 함수꼴, 파라미터, 확률변수의 정의역에 의해 값이 크게 달라진다는 것을 항상 기억해야 한다.
  - 함수꼴에 의해 값이 크게 달라지는 예는 위에서 보였다.
  - 파라미터에 의해서도 값이 크게 달라진다. 예를들어 $\phi(x;\beta)=\exp(-\beta x^2)$인 경우를 생각해보자.
    - $\beta \leq 0$인 경우 $Z$가 발산
    - $\beta > 0$인 경우 $p(x)$는 Gaussian distribution.
  - 확률 변수의 정의역에 의해서도 $Z$와 $p(x)$의 값이 달라진다. $n$차원 확률변수 $\vec{x}$에 대해 $\phi^{(i)}(x_i)=\exp(b_i x_i)$인 경우,
    - $\vec{x} \in \{0, 1\}^n$이면 $p(x_i=1)=\text{sigmoid}(b_i)$.
    - $\vec{x} \in \{[1, 0, ..., 0], [0, 1, ..., 0], ..., [0, 0, ..., 1]\}$이면 $p(\vec{x})=\text{softmax}(\vec{b})$.


### 16.2.4 Energy-Based Models
- Undirected model은 수학적으로 $\tilde{p}(\vec{x}) > 0$라는 가정이 필요하다.
- Energy-based model은 이러한 조건을 만족시키는 쉬운 방법 중 하나이다.
  - $\tilde{p}(\vec{x}) \exp{(-E(\vec{x}))}$
  - 위 식은 Boltamann distribution이기 때문에 Boltzmann machine이라고도 불린다.
  - $E(\vec{x})$ 앞의 $-$는 통계물리학의 컨벤션을 따르기 위해 있는 것이다. 머신러닝 관점에서는 꼭 있을 필요가 없지만(모델 입장에서는 $E(\vec{x})$의 부호를 뒤집어서 학습하면 됨) 보통 $-$를 붙여서 쓴다.

- Energy-based model을 사용하면 학습이 간단해진다.
  - Clique potential을 직접 학습하는 경우 $\tilde{p}(\vec{x}) > 0$를 제약조건으로 가지는 constrained optimization을 해야 한다.
  - 반면 energy-based model을 사용하여 $E(\vec{x})$를 학습하는 경우 unconstrained optimization을 할 수 있다.

- 많은 경우 $p_{\text{model}}$를 계산하지 않아도 된다. 대신 $-\log \tilde{p}(\vec{x})$꼴을 많이 사용한다.
  - 특히 latent variable이 있는 경우 $-\log \tilde{p}(\vec{x})$를 free energy라고 부른다.
  - $F(\vec{x}) = -\log \tilde{p}(\vec{x}) = -\log \sum_{\vec{h}}\exp(-E(\vec{x}, \vec{h}))$
