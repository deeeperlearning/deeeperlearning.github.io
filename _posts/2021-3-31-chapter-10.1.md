# 10 Sequence Modeling: Recurrent and Recursive Nets

CNN은 어떤 격자형태의 이미지를 처리하는데 특화된 모델이라면, RNN은 순차적인 데이터(sequential data)를 처리하는데 특화되어있다.

- Parameter sharing

    서로 다른 길이를 가진 데이터들에 대해서도 동일한 모델을 사용할 수 있다. 특히 어떤 특정한 정보가 sequence의 여러 위치에서 나타날 수 있을때 효과적이다.

    예를 들어, "나는 네팔에 2009년에 갔다." 와 "2009년도에 나는 네팔에 갔다." 라는 두 문장에서 네팔에 간 연도를 추출하는 문제를 생각해보자. 기존의 feedforward network는 parameter들이 분리되어 있기 때문에 각 단어가 서로 다른 위치에서 어떤 역할을 하는지에 대해 모든 경우의 수를 학습해야 하지만, parameter를 공유하는 경우에는 그럴 필요가 없다.

    Parameter sharing을 하는 간단한 예로 1-D temporal sequence를 매 타임스텝마다 동일한 커널을 쓰는컨볼루션 네트워크다. 이 경우 출력 데이터의 각각의 노드는 입력데이터의 인접한 몇개의 노드에 대한 함수가 된다. 

    Recurrent network는 조금 다른 방식으로 parameter sharing을 하는데, 출력 데이터의 각각의 노드는 이전 레이어의 출력노드들의 함수가 되고, 각각의 출력 노드는 이전 레이어에서와 동일한 update rule을 통해 구해진다.

## 10.1 Unfolding Computational Graphs

과거 데이터로부터 미래의 데이터를 예측하는 것을 생각해보면, 시간 $t$까지의 모든 데이터($\boldsymbol x^{(t)},\boldsymbol x^{(t-1)},\boldsymbol x^{(t-2)},...,\boldsymbol x^{(1)}$)의 정보를 저장하여 시간 $t+1$ 의 데이터를 예측하는 과정을 생각할 수 있다. 그러나 만약 시간 $t$까지의 데이터를 잘 요약해서 만들어진 일정한 길이를 가지는 벡터 $\boldsymbol h^{(t)}$를 만들 수 있다면 이것을 이용하는 것이 더 효과적일 것이다.

$$\boldsymbol h^{(t)} = f(\boldsymbol h^{(t-1)},\boldsymbol x^{(t)};\boldsymbol \theta)$$

아래 그림은 위의 식을 computational graph로 나타낸 것이다.

![_config.yml]({{ site.baseurl }}/assets/ch9/Fig10_2.png)

왼쪽의 그래프(circuit diagram)은 cycle이 있는 형태이고, 오른편(unfolded computational graph)은 acyclic 그래프이다. 검은 상자는 delay of 1 time step을 의미한다.

왼쪽 그래프를 오른쪽 그래프처럼 각 time step에서의 값을 하나의 노드로 나타내는 방법을 unfolding이라고 한다.

$$\boldsymbol h^{(t)}=g^{(t)}(\boldsymbol x^{(t)},\boldsymbol x^{(t-1)},\boldsymbol x^{(t-2)},...,\boldsymbol x^{(1)})  = f(\boldsymbol h^{(t-1)},\boldsymbol x^{(t)};\boldsymbol \theta)$$

$t$ step 이후의 recurrence는 과거의 모든 시퀀스를 받아오는 함수 $g^{(t)}$로 쓸 수 있지만, unfolding을 하게되면 $g^{(t)}$를 함수 $f$에 대해서 factorize할 수 있게된다. Unfolding을 하면서 얻게되는 이점은 크게 두가지 정도가 있다.

- 더이상 시퀀스 길이에 의존하지 않게 되면서, 학습된 모델은 항상 동일한 입력 사이즈를 갖게된다.
    - Training 데이터에 등장하지 않은 시퀀스 길이를 가진 데이터에 대해서도 일반화가 가능하다.
- 매 타임스텝마다 동일한 파라미터를 가진 함수 $f$를 사용할 수 있다.