# 8.Optimization for Training Deep Models

진행 순서

- 학습 알고리즘에서 사용하는 최적화 방법이 순수한(?) 최적화 알고리즘과 어떻게 다른지.
- 신경망을 최적화하는데 어려움을 주는 현실적인 문제들.
- 최적화를 위한 몇가지 알고리즘.
- 적응형 학습률 또는 비용함수의 2차 미분을 사용하는 최적화 알고리즘.
- 결론 및 리뷰

## 8.1 How Learning Differs from Pure Optimization

대부분의 머신러닝은 training set을 통해 학습한 후 test set을 통해 performance measure(P)를 측정한다. 즉, training set으로부터 계산된 비용함수를 줄이면 test set에 대한 비용함수도 줄어들 것이라는 기대만으로 최적화를 진행하기 때문에 순수한 최적화와는 차이가 있다.

### 8.1.1 Empirical Risk Minimization

실제 데이터를 생성하는 확률분포를 $p_{data}(\boldsymbol x,y)$라 하면, 최적화의 최종 목적은 아래의 목적함수를 최소화 하는것이다.

$$J^*(\boldsymbol \theta) = \mathbb E_{(\boldsymbol x,y)\sim p_{data}}L(f(\boldsymbol x;\boldsymbol \theta),y)$$

그러나 $p_{data}(\boldsymbol x,y)$를 알 수 없기 때문에 training set의 확률분포에 대해서 최적화를 하는데, 이를 empirical risk minimization이라고 한다.

$$\mathbb E_{(\boldsymbol x,y)\sim \hat p_{data}}L(f(\boldsymbol x;\boldsymbol \theta),y) = \frac{1}{m}\sum^{m}_{i=1}L(f(\boldsymbol x^{(i)};\boldsymbol \theta),y^{(i)})$$

- 오버피팅이 생기기 쉽다.
- 대부분의 알고리즘은 경사하강법을 사용하는데, 0-1 loss 같은 유용한 손실함수는 기울기를 측정할 수 없다.

위의 두가지 이유로 딥러닝에서는 empirical risk minimization을 잘 사용하지 않는다고 한다.

### 8.1.2 Surrogate Loss Functions and Early Stopping

손실함수를 직접 사용하지 않고, 대리함수를 최소화하는 방법.

- 예를 들어 0-1 loss의 경우 negative log-likelihood를 사용하여 최적화(다른 대리 함수도 많음).
    - 0-1 loss를 정확하게 최소화하는 것은 다루기 어렵기 때문.
    - 또한, 어떤 경우에는 training set의 0-1 loss가 0에 도달했음에도 불구하고 log-likelihood를 학습 시킴으로써 test set의 loss를 더 감소시킬 수 있다.

머신러닝에서의 최적화와 일반적인 최적화 사이의 또 하나의 차이점은 local minimum에서 멈추지 않는다는 것이다. 그 이유는 수렴의 표준이 early stopping을 따르기 때문이고, 따라서 gradient가 충분히 큼에도 불구하고 학습이 종료될 수도 있다.

### 8.1.3 Batch and Minibatch Algorithm

머신러닝에서의 최적화와 일반적인 최적화이 다른 이유중 하나는 목적함수를 training examples에 대한 합으로 분해한다는 것이다.

- 아래와 같은 MLE 문제를 생각해보면 최적화 해야하는 목적함수와 그에 대한 gradient는 아래와 같다.

    $$\boldsymbol \theta_{ML} = \argmax_{\boldsymbol \theta}\sum^m_{i=1}\log p_{model}(\boldsymbol x^{(i)},y^{(i)},\boldsymbol \theta)$$

    $$J(\boldsymbol \theta) = \mathbb E_{\boldsymbol x,y \sim \hat p_{data}}\log p_{model}(\boldsymbol x,y;\boldsymbol\theta)\ ,\ \ \nabla_{\boldsymbol \theta}J(\boldsymbol \theta) = \mathbb E_{\boldsymbol x,y \sim \hat p_{data}}\nabla_{\boldsymbol \theta}\log p_{model}(\boldsymbol x,y;\boldsymbol\theta)$$

    - 모든 dataset을 한번에 사용(batch)하여 gradient를 계산하는 것은 cost가 너무 크다.
    - $n$개의 데이터에 대한 평균의 standard error는 $\sigma/\sqrt{n}$ 으로 주어지기 때문에, 데이터양을 늘리는 것에 비해 standard error의 감소량이 작다.
    - 따라서 standard error가 조금 크더라도 랜덤하게 샘플링된 작은 dataset를 사용(minibatch)하여 학습하는 것이 수렴이 빠름.(Minibatch의 size를 1로 두는 것을 stochastic  또는 online method라고 한다)
- Minibatch를 사용하는 또 다른 이유는 identical examples에 대한 gradient 계산량을 줄일 수 있다는 점이다. 모든 데이터 셋을 한번에 사용하면 똑같은 gradient 계산을 여러번 해야하지만, minibatch를 사용하면 수렴이 빠르기 때문에 중복된 계산을 줄일 수 있다.
- Minibatch size를 결정하는데 고려해야할 것들.
    - 크면 클수록 정확하지만 효율이 좋지 않음.
    - 멀티코어의 경우 너무 작은 batch size는 활용률이 낮다.
    - Batch size를 크게 해야할 경우 큰 메모리가 필요하다.
    - GPU를 사용할 경우 batch size는 2의 제곱수로 결정하는게 좋다.
    - 작은 batch size는 정칙화 효과를 가져올 수 있다.(Wilson and Martinez, 2003) 아마 작은 batch size로부터 발생하는 noise때문일 것으로 예상되는데, 그렇기 때문에 작은 batch size일수록 학습률을 낮춰야 한다.
- Gradient만을 사용하여 학습시키는 알고리즘의 경우 batch size는 100미만으로 작게해도 되지만, Hessian matrix가 사용되는 알고리즘의 경우는 gradient의 작은 변화로도 결과를 크게 바꿀 수 있기 때문에 batch size를 크게 키울 필요가 있다.
- Minibatch를 랜덤하게 샘플링하는게 매우 중요하다. Gradient를 unbiased estimation을 하기 위해선 데이터들이 독립이어야 하지만 많은 데이터셋이 그렇지 않기때문.
    - 매우 큰 데이터 셋의 경우, minibatch를 뽑을 때마다 데이터셋을 셔플할 필요는 없고 처음 한번만 셔플해도 충분하다고 함.
- 첫번째 에폭에서는 unbiased하게 모델이 학습되지만 두번째 에폭부터는 데이터들이 재사용되기 때문에 bias가 생길 수 밖에 없다. 그러나 전체 데이터셋을 여러번 학습함으로써 training error가 줄어드는 이득을 볼수 있기 때문에 에폭수를 늘리는 것이 좋다.(물론 early stopping이 허용하는 한에서)

## 8.2 Challenges in Neural Network Optimization

### 8.2.1 Ill-conditioning

오목 함수를 최적화 하는 과정에서 많은 문제가 발생하는데, 대표적인 예로 Hessian matrix로 인한 문제가 있다.

어떤 함수를 2차 테일러 시리즈로 근사하면 아래와 같이 쓸수 있는데,

$$f(\boldsymbol x)=f(\boldsymbol x^{(0)}) +(\boldsymbol x-\boldsymbol x^{(0)})^{\top}\boldsymbol g +\frac{1}{2}(\boldsymbol x-\boldsymbol x^{(0)})^{\top}\boldsymbol H(\boldsymbol x-\boldsymbol x^{(0)})$$

이를 SGD를 이용하여 최적화 시킨다고 하면 다음 스텝에선 $\boldsymbol x = \boldsymbol x^{(0)}-\epsilon \boldsymbol g$이므로,

$$f(\boldsymbol x^{(0)}-\epsilon \boldsymbol g)\approx f(\boldsymbol x^{(0)})-\epsilon \boldsymbol g^{\top} \boldsymbol g +\frac{1}{2}\epsilon^2 \boldsymbol g^{\top}\boldsymbol H \boldsymbol g $$

문제는 $\frac{1}{2}\epsilon^2 \boldsymbol g^{\top}\boldsymbol H \boldsymbol g$ 가 $\epsilon \boldsymbol g^{\top}\boldsymbol g$ 보다 커지는 상황이다. 따라서 이를 방지하려면 학습률 $\epsilon$이 작아져야 하고, 따라서 학습이 매우 느려진다.

위와같이 Hessian matrix로 인해 발생하는 문제를 해결하는데 좋은 방법으로 Newton's method가 있지만 이 방법 또한 뉴럴넷에 적용하기 위해선 수정이 필요하다고 한다.

### 8.2.2 Local Minima

뉴럴넷은 non-convex 비용함수를 가지기 때문에 많은 local minima를 가질 수 있는데, 사실 모든 deep model들은 무수히 많은 local minima를 가진다고 한다.

- deep model같은 경우 서로 다른 weights를 가진 모델이더라도 동일한 output을 낼 수 있기때문에(weight space symmetry) 두 모델을 식별할 수 없고 따라서 매우 많은 local minima를 가지게 된다.

local minima에서의 비용함수 값이 global minima에서의 값과 차이가 많이 난다면 큰 문제지만, 충분히 큰 모델에서는 대부분의 local minima가 낮은 비용함수 값을 가지기 때문에 별로 문제가 되지 않는다고 한다. 

그렇지만 high-dimension에선 local minima에 빠졌다는 것을 알기가 매우 어렵기 때문에 여전히 많은 사람들이 고통받고 있다.

### 8.2.3 Plateaus, Saddle Points and Other Flat Regions

- 램덤 함수(실험에 의해 구해진 값이 argument가 되는 함수)는 아래와 같은 특징을 가진다.
    - low-dimension에서는 saddle point 보다 local minima가 더 자주 발생하지만, high-dimension에서는 saddle point가 더 많이 생긴다.
        - Local minima에서는 Hessian matrix의 고유값들의 부호가 모두 양수이지만 saddle point에서는 음수 양수가 섞여있기 때문에, dimension이 높아질수록 saddle point가 발생할 확률이 더 높다.
    - local minima에선 낮은 비용함수 값을, saddle point에선 상대적으로 높은 비용함수 값을 가질 가능성이 높다.
- 실제로 뉴럴넷에서도 위와같은 특징이 발견된다고한다. (BaldiandHornik(1989), Saxe et al. (2013), Dauphin et al. (2014), Choromanska et al. (2014))
- Saddle point가 증가하면 first-order optimization이 위험할 수 있지만,  경험으로 미루어 보면 1차 최적화를 사용해도 saddle point를 잘 지나간다고 한다.
- 기울기가 0이 되는 지점은 local maxima도 있고 wide flat region도 있을수 있는데, local maxima는 극히 드물게 발생하기 때문에(local minima와 같은 이유) 문제가 되지 않지만 wide flat region같은 경우 오목 함수가 아닌 이상 문제가 어려워진다.

### 8.2.4 Cliffs and Exploding Gradients

![_config.yml]({{ site.baseurl }}/assets/ch8/Fig8_3.png)

뉴럴넷에서는 종종 위와같은 기울기 절벽(?) 이 발생하는데, 경사하강법을 그대로 쓰게되면 이상한 위치로 튈 수 있기 때문에 weight clipping을 해줘야 한다. 이러한 절벽은 RNN에서 자주 발생한다고 한다.

### 8.2.5 Long-Term Dependencies

Computational graph가 너무 깊어져도 문제가 발생한다. 특히 RNN의 경우, 동일한 연산이 반복되기 때문에 더 취약하다. Matrix $\boldsymbol W$를 곱하는 연산이 연속적으로 $t$번 행해지는 computational graph가 있다고 하면 결과적으로 $\boldsymbol W^t$가 곱해진다. $\boldsymbol W$를 eigen-decomposition하게 되면 $\boldsymbol W^t = \boldsymbol V \text{diag}(\boldsymbol\lambda)^t\boldsymbol V^{-1}$ 이 되는데, 이때 $\boldsymbol\lambda$가 1보다 큰지 작은지에 따라 기울기가 explode하거나 vanish한다.

### 8.2.6 Inexact Gradients

대부분의 최적화 알고리즘은 정확한 기울기나 정확한 Hessian matrix를 알고있다는 가정에서 시작하는데, 현실에서 최적화를 진행할 때는 여러가지 이유로 minibatch를 사용한다. 즉, 기울기가 biased 되어있다. 또한 기울기를 구할 수 없는 경우도 발생한다. 이런 경우 대체 비용함수를 사용해서 해결 가능하다.

### 8.2.7 Poor Correspondence between Local and Global Structure

![_config.yml]({{ site.baseurl }}/assets/ch8/Fig8_4.png)

지금까지는 한 점에서 발생하는 문제들을 봤는데(local minima, saddle point ...), 이 모든 문제점들을 해결해도  발생하는 문제가 있다.

- 위의 그림에선 local minima나 saddle point가 없음에도 불구하고 최적화가 안되는 것을 볼 수 있는데, 이러한 문제가 발생하는 이유는 대부분의 최적화 알고리즘이 small, local move를 사용하기 때문이다.
- 현실에서는 inexact gradient를 사용하기 때문에 올바른 방향으로 학습이 되고있는지도 장담할 수 없다.
- 또한 굉장히 비효율적인 길을 따라 최적화가 진행될 수도 있다.

아직 비용함수의 global structure를 사용하거나 non-local move를 이용하는 알고리즘에 대한 연구가 덜 되어서 대부분 initial condition을 조정해가며 해결한다고 한다.

### 8.2.8 Theoretical Limits of Optimization

- 몇몇 연구에서는 이론적 결과는 오직 이산값을 주는 신경망에만 적용 가능하다고 하지만, 대부분의 신경망은 로컬 검색을 통해 최적화 할 수 있도록 smooth하게 증가하는 값을 출력한다.
- 어떤 연구결과에서는 네트워크 사이즈가 정해져있을 때 솔루션을 찾는 것은 매우 어려운 문제이다.
    - 현실에서는 그냥 네트워크 사이즈를 키워서 해결할 수 있고, 정확한 minimum을 찾지 않아도 충분히 좋은 성능을 낼 수 있기 때문에 큰 문제가 아니다.

최적화 알고리즘에 대한 이론적 분석은 매우 어려운 작업이지만 최적화 알고리즘의 성능에 대한 현실적인 bound에 대한 연구는 중요한 과제이다.