### 12.4.3.3 Importance Sampling

NLP training을 가속하는 방법중 하나로 현재 문맥의 다음 위치에 나타날 가능성이 매우 적은 단어들에 대해 gradient contribution 계산을 생략하는 것이다. 단어를 일일이 나열해 확률을 계산하는 대신 그 단어들의 부분집합만 추출하는 것이다. 

![chapter%2012%204%20NLP%209c4bec19ceb342de9c7f6d63ac82fbeb/Untitled.png](chapter%2012%204%20NLP%209c4bec19ceb342de9c7f6d63ac82fbeb/12.4nlp01.png)

마지막 softmax output 이전의 hidden layer이 위와 같이 affine transformation이라고 한다면, gradient를 아래와 같이 표기할 수 있다. 

![_config.yml]({{ site.baseurl }}/assets/ch12/12.4nlp02.png)

a는 softmax layer 통과 이전의 벡터(혹은 score)값이며, 이 벡터는 단어 하나당 하나의 성분으로 구성된다. C는 문맥, 그리고 y는 다음에 올 단어를 의미한다. 위 식의 마지막 줄에서 앞의 항은 positive phase라고 부르며 이 항은 $a_y$를 위로 밀어 올린다. 두번째 항은 negative phase라고 부르고 모든 i에 대에 weight P(y=i|c)를 이용해 기울기를 아래로 끌어내린다. negative phase는 하나의 기댓값이므로 montecarlo sampling를 이용해 추정할 수 있다. 그러려면 model 자체에서 sample을 추출해야 하는데, 이는 모든 i에 대해 P(i|C)를 계산하는 비싼 일이다. 

모형 대신 proposal distribution이라는 다른 분포 q에서 분포를 추출할 수도 있다. 이런 분포(보통 unigram, bigram 분포)를 사용함으로서 발생하는 편향은 적절한 weight를 적용해 바로잡으면 된다. 부정적 단어 $n_i$가 추출되었을 때 그에 해당하는 gradient에 적용되는 weight는 아래와 같다. 

![_config.yml]({{ site.baseurl }}/assets/ch12/2.4nlp03.png)

$p_i$= P(i|C)다. 이를 이용해 softmax층의 gradient를 다시 기술하면 아래와 같다. 

![chapter%2012%204%20NLP%209c4bec19ceb342de9c7f6d63ac82fbeb/Untitled%203.png](chapter%2012%204%20NLP%209c4bec19ceb342de9c7f6d63ac82fbeb/12.4nlp04.png)

![_config.yml]({{ site.baseurl }}/assets/ch12/12.4nlp05.png)

bigram distribution

### 12.4.3.4 Noise-Contrastive Estimation and Ranking Loss

큰 사이즈의 vocabulary에 대한 NLP model을 훈련하는 데 필요한 계산 비용을 줄이는 다른 예로 Ranking loss를 들 수 있다. Ranking loss는 각 단어에 대한 NN 모형의 출력을 하나의 score로 간주해 정확한 단어가 다른 단어들보다  더 높은 순위가 되도록 그 단어의 점수 $a_y$를 다른 단어들의 점수 $a_i$보다 높게 책정한다. 그리고 아래 식으로 ranking loss값을 계산한다.  

![_config.yml]({{ site.baseurl }}/assets/ch12/12.4nlp06.png)

관측된 단어의 점수  $a_y$가 부정적 단어의 점수인 $a_i$보다 1 이상 크면 i번째 항에 대한 기울기가 0이 된다. 

### 12.4.4 Combining Neural Language Models with n-grams

NN 기반 model 대비 n-gram model이 가지는 주요 장점은 capacity가 높으면서도 각 sample을 처리하는데 드는 비용이 적다는 것이다. (현재 문맥에 부합하면 소수 tuple만 참조하면 됨) 다시 말해 capacity과 training cost가 서로 독립적이다.  

두 접근 방식을 결함해 계산량을 크게 높이지 않고 capacity를 높이는 것이 가능하다. Bengio 2001, 2003은 NN model과 n-gram model을 ensemble로 구성했다. 두 모델의 오차가 서로 독립이라면 ensemble model을 통해 test error가 감소할 것이다.

### 12.4.5 Neural Machine Translation

어떤 자연어로 된 글/문장을 다른 자연어로 된 같은 뜻의 문장으로 변환하는 task이다. 

Devlin 2014에서는 원본 언어문구 $s_1, s_2...$에 대한 대상 언어 문구 $t_1, t_2...$에 하나의 MLP를 이용해 점수를 부여하는 방법을 사용했다. 이 MLP는 $P(s_1, s_2 ...$|$t_1, t_2...)$를 추정한다. 이 MLP 접근법의 단점은 sequence를 고정된 길이로 만드는 전처리 과정이 필요ㅎ다는 점이다. 좀 더 유연한 input, output을 지원하는 모델이 더 좋을 것 같은데, RNN이 이런 능력을 제공한다. CNN을 사용하는 모델도 있다. 

![_config.yml]({{ site.baseurl }}/assets/ch12/12.4nlp07.png)

  

### 12.4.5.1 Using an Attention Mechanism and Aligning Pieces of Data

Attention mechanism은 machine translation을 위해 만들어진 구조이다. 일반적인 sequence to sequence model은 target language의 길이가 길수록 낮은 성능을 보인다. 이를 극복하기 위해 모델이 중요한 부분만 잘 기억하도록 만드는 것이 Attention mechanism의 핵심 아이디어이다. 

sequence to sequence 모델은 글자, 단어 등의 sequence를 받아 다른 아이템의 sequence를 출력한다. 

![_config.yml]({{ site.baseurl }}/assets/ch12/12.4nlp08.png)

machine translation의 경우 context가 하나의 vector 형태로 전달되는데, 이때 encoder, decoder은 보통 RNN으로 구성된다. seq2seq model에서 RNN은 한 timestep마다 지금 들어온 input과 이전 timestep의 hidden input을 받는다. 이런 모델의 단점은, 긴 input의 경우 전체 맥락을 고정된 context vector로 적절히 나타내기 어렵다는 것이다. 

Attention mechanism은 seq2seq 모델이 decoding 과정에서 현재 스텝과 가장 연관성이 깊은 파트에 집중할 수 있도록 유도하여 이 문제를 해결한다. 예를 들어 '나는 오이와 민트초코를 오이를 좋아해'를 'I love cucumber and mintchoco.'로 번역하는 과정중 decoding 과정에서 "cucumber"이라는 단어를 만들 때 "오이"라는 매칭되는 표현에 더 집중하도록 하는 것이다. 

Attention mechanism에서는 기존 seq2seq model과 달리 모든 step의 hidden state를 decoder에 넘겨준다. 그리고 decoding 단계에서 전체 hidden state를 보고 각 hidden state마다 점수를 매긴다. 이후 매겨진 점수에 softmax를 취해 각 timestep의 hidden state에 곱한다. 그로인해 더 높은 점수를 가진 hidden state는 더 큰 부분을 차지하게 된다. 

![_config.yml]({{ site.baseurl }}/assets/ch12/12.4nlp09.png)

이렇게 점수를 매기는 과정은 decoder가 단어를 생성하는 매 스탭마다 반복된다. 

![_config.yml]({{ site.baseurl }}/assets/ch12/12.4nlp10.png)



### 12.4.6 역사적 관점

NLP에 깔린 아이디어들은 다양한 응용들로 확장되었다. 

- 파싱
- 품사분석
- semantic role labeling
- 다중 과제 학습 아키텍쳐
- t-SNE가 언어 모형 분석 도구로 사용되기도 함.



references

[https://ratsgo.github.io/from frequency to semantics/2017/10/06/attention/](https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/10/06/attention/)

[https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)